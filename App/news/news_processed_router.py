from fastapi import APIRouter, HTTPException, Query
from Database.mongodb_connection import mongo_db
import os
import time
import pandas as pd
import re
from tqdm import tqdm  # âœ… tqdm ì¶”ê°€
from Preprocessor_NFdata.Preprocess_news import run_preprocess_naver
from Preprocessor_NFdata.Preprocess_tfidf import run_semi_tfidf
# from Preprocessor_NFdata.Preprocess_tfidf_tokenized import run_tfidf
from Crawler.naver_news_crawler import run_crawler

# FastAPI ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/news",
    tags=["news"]
)

def extract_year_from_filename(filename):
    """ ğŸ“Œ íŒŒì¼ëª…ì—ì„œ YYYY ì—°ë„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ """
    match = re.search(r'(\d{4})', filename)  # 4ìë¦¬ ìˆ«ì ì°¾ê¸°
    return int(match.group(1)) if match else None  # ì°¾ìœ¼ë©´ int ë³€í™˜ í›„ ë°˜í™˜, ì—†ìœ¼ë©´ None

@router.post("/preprocess/news")
def preprocess_news(
    mode: str = Query("local", description="ì‹¤í–‰ ëª¨ë“œ: 'local' ë˜ëŠ” 'crawler'"),
    years: str = Query(None, description="ì²˜ë¦¬í•  ì—°ë„ ë¦¬ìŠ¤íŠ¸ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´)")
):
    """
    ğŸ“° ë„¤ì´ë²„ ì£¼ì‹ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    
    - **mode="local"** â†’ ë¡œì»¬ íŒŒì¼ì„ ì½ì–´ì„œ ì „ì²˜ë¦¬ í›„ DBì— ì €ì¥
    - **mode="crawler"** â†’ í¬ë¡¤ëŸ¬ì—ì„œ ë°”ë¡œ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ì „ì²˜ë¦¬ í›„ DBì— ì €ì¥
    - **years="2022,2023"** â†’ íŠ¹ì • ì—°ë„(YYYY)ë§Œ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬
    """
    preprocessed_news_collection = mongo_db.preprocessed_news  # ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ ì»¬ë ‰ì…˜

    # ğŸ“Œ ì„ íƒí•œ ì—°ë„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ (ì˜ˆ: "2022,2023" â†’ [2022, 2023])
    selected_years = [int(year.strip()) for year in years.split(",")] if years else None

    if mode == "local":
        # ğŸ“Œ ë¡œì»¬ íŒŒì¼ ëª¨ë“œ
        raw_path = os.path.join(os.getcwd(), "Non_Finance_data", "Naver_Stock")  # ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„° ê²½ë¡œ
        files = [os.path.join(raw_path, file) for file in os.listdir(raw_path)]

        category_path = os.path.join(os.getcwd(), "Database", "sector_vocab")  # ì‚°ì—…ë³„ ë‹¨ì–´ ì‚¬ì „ ê²½ë¡œ
        category_files = {file.split('.')[0]: os.path.join(category_path, file) for file in os.listdir(category_path)}

        if not files:
            raise HTTPException(status_code=404, detail=f"No raw news found in '{raw_path}'")

        for file in files:
            file_year = extract_year_from_filename(file)  # ğŸ“Œ ì—°ë„ ì¶”ì¶œ

            # ğŸ›‘ ì—°ë„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ìŠ¤í‚µ
            if file_year is None:
                print(f"â© Skipping {file} (Year: None)")
                continue

            # ğŸ›‘ ì—°ë„ê°€ ì„ íƒëœ years ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if selected_years and file_year not in selected_years:
                print(f"â© Skipping {file} (Year: {file_year})")
                continue

            print(f"âœ… Processing {file} (Year: {file_year})")

            df = pd.read_csv(file, encoding='utf-8-sig', on_bad_lines="skip")
            try:
                _ = run_preprocess_naver(df)
                processed_news = run_semi_tfidf(_, category_files)
                
                if processed_news is not None:
                    if isinstance(processed_news, tuple):
                        processed_news = processed_news[0]
                    
                    # âœ… `_id`ë¥¼ `Link` ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
                    records = [{**record, "_id": record["Link"]} for record in processed_news.to_dict('records')]

                    saved_count = 0  # ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜
                    duplicate_count = 0  # ì¤‘ë³µìœ¼ë¡œ ì €ì¥ë˜ì§€ ì•Šì€ ë°ì´í„° ê°œìˆ˜

                    # âœ… tqdmìœ¼ë¡œ ì €ì¥ ì§„í–‰ë¥  í‘œì‹œ & ì¤‘ë³µ ë°©ì§€ ì²˜ë¦¬
                    for record in tqdm(records, desc=f"Saving {file} to MongoDB", unit="doc"):
                        existing_count = preprocessed_news_collection.count_documents({"_id": record["_id"]})

                        if existing_count == 0:
                            preprocessed_news_collection.insert_one(record)
                            saved_count += 1
                        else:
                            duplicate_count += 1  # ì¤‘ë³µ ë°ì´í„° ê°œìˆ˜ ì¦ê°€

                    print(f"âœ… {file} processed: ì´ {len(df)}ê°œ ì¤‘ {saved_count}ê°œ ì €ì¥ë¨, {duplicate_count}ê°œ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ì•ˆë¨")
                else:
                    print(f"âš ï¸ {file} processed: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì—†ìŒ")
            except Exception as e:
                print(f"âš ï¸ {file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    elif mode == "crawler":

        category_path = os.path.join(os.getcwd(), "Database", "sector_vocab")  # ì‚°ì—…ë³„ ë‹¨ì–´ ì‚¬ì „ ê²½ë¡œ
        category_files = {file.split('.')[0]: os.path.join(category_path, file) for file in os.listdir(category_path)}

        # âœ… í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° MongoDB ì €ì¥
        print("ğŸš€ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        run_crawler(save_to_db=True)

        raw_news_collection = mongo_db.raw_news

        # âœ… DB ë°˜ì˜ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
        timeout = 30
        elapsed = 0
        while raw_news_collection.count_documents({}) == 0 and elapsed < timeout:
            print("â³ Waiting for MongoDB to reflect raw_news data...")
            time.sleep(2)
            elapsed += 2

        if raw_news_collection.count_documents({}) == 0:
            raise HTTPException(status_code=500, detail="â›” í¬ë¡¤ëŸ¬ ì‹¤í–‰ í›„ì—ë„ raw_newsì— ë°ì´í„°ê°€ ì—†ìŒ")

        # âœ… MongoDBì—ì„œ `raw_news` ê°€ì ¸ì˜¤ê¸°
        raw_data = list(raw_news_collection.find())

        if not raw_data:
            raise HTTPException(status_code=500, detail="â›” raw_newsì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")

        print(f"âœ… raw_news ë°ì´í„° ê°œìˆ˜: {len(raw_data)}ê°œ")

        # âœ… `_id`ë¥¼ `Link` ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        for record in raw_data:
            record["_id"] = record["Link"]

        # âœ… ì „ì²˜ë¦¬ ë° TF-IDF ì ìš©
        _ = run_preprocess_naver(pd.DataFrame(raw_data))
        processed_news = run_semi_tfidf(_, category_files)

        if processed_news is not None:
            if isinstance(processed_news, tuple):
                processed_news = processed_news[0]

            records = [{**record, "_id": record["Link"]} for record in processed_news.to_dict('records')]

            saved_count, duplicate_count = 0, 0

            for record in tqdm(records, desc="Saving processed news to MongoDB", unit="doc"):
                if preprocessed_news_collection.count_documents({"_id": record["_id"]}) == 0:
                    preprocessed_news_collection.insert_one(record)
                    saved_count += 1
                else:
                    duplicate_count += 1

            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(raw_data)}ê°œ ì¤‘ {saved_count}ê°œ ì €ì¥ë¨, {duplicate_count}ê°œ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ì•ˆë¨")

        # âœ… raw_news ì»¬ë ‰ì…˜ ì‚­ì œ
        print("ğŸ—‘ï¸ Deleting raw_news collection...")
        raw_news_collection.drop()
        print("âœ… raw_news ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ!")

    else:
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ëª¨ë“œì…ë‹ˆë‹¤. 'local' ë˜ëŠ” 'crawler'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

    return {"message": f"âœ… News processing completed using mode: {mode}, years: {selected_years}"}