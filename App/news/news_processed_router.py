import json
import os
import pandas as pd
from tqdm import tqdm  # âœ… tqdm ì¶”ê°€
from fastapi import APIRouter, HTTPException, Query
from Database.mongodb_connection import mongo_db
from LLM_modeling.vectorize.article_summarize import NewsTokenizer

# ğŸš€ FastAPI ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/summary",
    tags=["summary"]
)

@router.post("/summarize/data")
def summarize_and_vectorize_news(mode: str = Query("db", description="ì‹¤í–‰ ëª¨ë“œ: 'local' ë˜ëŠ” 'db'")):
    """
    ğŸ“° ë‰´ìŠ¤ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³  ë²¡í„°í™”í•˜ì—¬ MongoDBì— ì €ì¥í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    
    - **mode="local"** â†’ ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ìš”ì•½ í›„ ì €ì¥
    - **mode="db"** â†’ MongoDBì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ìš”ì•½ í›„ ì €ì¥
    """
    summary_collection = mongo_db.summary_and_vectors  # âœ… ìš”ì•½ ê²°ê³¼ ì €ì¥í•  MongoDB ì»¬ë ‰ì…˜
    news_tokenizer = NewsTokenizer(
        peft_model_dir="LLM_modeling/finetuning/mt5_large_peft_final",
        dataset_file="dummy",
        output_file="LLM_modeling/backup/summary_backup.json"
    )

    batch_size = 3  # ğŸ”„ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_records = []  # ì €ì¥í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    processed_count = 0  # âœ… ì²˜ë¦¬ëœ ë¬¸ì„œ ê°œìˆ˜
    error_count = 0  # âŒ ì—ëŸ¬ ë°œìƒ ë¬¸ì„œ ê°œìˆ˜
    duplicate_count = 0  # ğŸ”„ ì¤‘ë³µëœ ë°ì´í„° ê°œìˆ˜

    def convert_oid(record):
        """MongoDB ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        rec = record.copy()
        rec["_id"] = str(rec["_id"])
        return rec

    if mode == "db":
        # âœ… MongoDBì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_collection = mongo_db.preprocessed_news
        documents = list(doc_collection.find())  # âœ… ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ tqdm ì ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

    elif mode == "local":
        # âœ… ë¡œì»¬ CSVì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        raw_path = os.path.join(os.getcwd(), "Non_Finance_data", "Naver_Stock")
        files = [os.path.join(raw_path, file) for file in os.listdir(raw_path) if file.endswith('.csv')]

        if not files:
            raise HTTPException(status_code=404, detail=f"No news files found in {raw_path}")

        all_data = []
        for file in files:
            df = pd.read_csv(file, encoding="utf-8-sig", on_bad_lines="skip")
            all_data.extend(df.to_dict('records'))

        documents = all_data  # âœ… MongoDBì—ì„œ ì½ì€ ë°ì´í„°ì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜

    else:
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ëª¨ë“œì…ë‹ˆë‹¤. 'local' ë˜ëŠ” 'db'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

    # ğŸ” ë¬¸ì„œ ìˆœíšŒí•˜ë©° ìš”ì•½ ë° ë²¡í„°í™” ì§„í–‰ (âœ… tqdm ì ìš©)
    for doc in tqdm(documents, desc="Processing news summarization", unit="doc"):
        doc_id = doc.get("Link", None)  # âœ… `_id`ë¥¼ `Link` ê°’ìœ¼ë¡œ ì„¤ì •
        if not doc_id:
            continue  # `_id`ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ

        if summary_collection.find_one({"_id": doc_id}):
            duplicate_count += 1  # âœ… ì¤‘ë³µ ê°œìˆ˜ ì¦ê°€
            continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ìŠ¤í‚µ

        try:
            article = doc.get("Body_processed", "") if mode == "db" else doc.get("Body", "")
            if not isinstance(article, str) or not article.strip():
                print(f"âš ï¸ Invalid or empty 'Body_processed' in document with _id {doc_id}. Skipping.")
                continue

            entity = {
                "id": str(doc_id),
                "article": article
            }
            result = news_tokenizer.summarize_and_tokenize(entity)  # ğŸ”„ ë‰´ìŠ¤ ìš”ì•½ ë° ë²¡í„°í™”
            result["_id"] = doc_id  # âœ… `_id`ë¥¼ `Link` ê°’ìœ¼ë¡œ ì„¤ì •

            batch_records.append(result)
            processed_count += 1

            # âœ… ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ DBì— ì €ì¥
            if len(batch_records) >= batch_size:
                for record in batch_records:
                    summary_collection.update_one(
                        {"_id": record["_id"]},
                        {"$set": record},
                        upsert=True
                    )
                with open(news_tokenizer.output_file, "a", encoding="utf-8") as f:
                    backup_records = [convert_oid(rec) for rec in batch_records]
                    json.dump(backup_records, f, ensure_ascii=False, indent=4)
                    f.write("\n")
                batch_records = []

        except Exception as e:
            print(f"âŒ Error processing document with _id {doc_id} - {e}")
            error_count += 1
            continue

    # ğŸ”„ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
    if batch_records:
        for record in batch_records:
            summary_collection.update_one(
                {"_id": record["_id"]},
                {"$set": record},
                upsert=True
            )
        with open(news_tokenizer.output_file, "a", encoding="utf-8") as f:
            backup_records = [convert_oid(rec) for rec in batch_records]
            json.dump(backup_records, f, ensure_ascii=False, indent=4)
            f.write("\n")

    print(f"âœ… ìš”ì•½ ì™„ë£Œ: ì´ {len(documents)}ê°œ ì¤‘ {processed_count}ê°œ ì €ì¥ë¨, {duplicate_count}ê°œ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ì•ˆë¨")

    return {
        "message": f"âœ… Summarization and vectorization completed using mode: {mode}",
        "processed": processed_count,
        "duplicates": duplicate_count,
        "errors": error_count
    }