import os
import json
import math
from datetime import datetime, timedelta
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline
from tqdm import tqdm
from collections import defaultdict

# âœ… ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INPUT_FILE = os.path.join(BASE_DIR, "38_KOTE_Input.json")
FE2_OUTPUT_FILE = os.path.join(BASE_DIR, "kf_deberta_kote_final_ver1.json")
FE2_DAILY_OUTPUT_FILE = os.path.join(BASE_DIR, "kf_deberta_kote_final_ver2.json")
KIND_FILE = os.path.join(BASE_DIR, "KIND_38.json")

# âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = os.path.join(BASE_DIR, "kf_deberta_kote_model", "final_model")
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained("kakaobank/kf-deberta-base")

# âœ… ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±
pipe = TextClassificationPipeline(
    model=model,
    tokenizer=tokenizer,
    device=0,  # ğŸ”¥ GPU ì‚¬ìš©
    top_k=None,  # âœ… ëª¨ë“  ê°ì • í™•ë¥  ì¶œë ¥
    function_to_apply='sigmoid'
)

# âœ… ê°ì • ì´ë¦„ ë§¤í•‘ (KOTE ê°ì • 44ê°œ)
LABELS = ["ë¶ˆí‰/ë¶ˆë§Œ", "í™˜ì˜/í˜¸ì˜", "ê°ë™/ê°íƒ„", "ì§€ê¸‹ì§€ê¸‹", "ê³ ë§ˆì›€", "ìŠ¬í””", "í™”ë‚¨/ë¶„ë…¸",
    "ì¡´ê²½", "ê¸°ëŒ€ê°", "ìš°ì­ëŒ/ë¬´ì‹œí•¨", "ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§", "ë¹„ì¥í•¨", "ì˜ì‹¬/ë¶ˆì‹ ", "ë¿Œë“¯í•¨",
    "í¸ì•ˆ/ì¾Œì ", "ì‹ ê¸°í•¨/ê´€ì‹¬", "ì•„ê»´ì£¼ëŠ”", "ë¶€ë„ëŸ¬ì›€", "ê³µí¬/ë¬´ì„œì›€", "ì ˆë§", "í•œì‹¬í•¨",
    "ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€", "ì§œì¦", "ì–´ì´ì—†ìŒ", "ì—†ìŒ", "íŒ¨ë°°/ìê¸°í˜ì˜¤", "ê·€ì°®ìŒ", "í˜ë“¦/ì§€ì¹¨",
    "ì¦ê±°ì›€/ì‹ ë‚¨", "ê¹¨ë‹¬ìŒ", "ì£„ì±…ê°", "ì¦ì˜¤/í˜ì˜¤", "íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)", "ë‹¹í™©/ë‚œì²˜",
    "ê²½ì•…", "ë¶€ë‹´/ì•ˆ_ë‚´í‚´", "ì„œëŸ¬ì›€", "ì¬ë¯¸ì—†ìŒ", "ë¶ˆìŒí•¨/ì—°ë¯¼", "ë†€ëŒ", "í–‰ë³µ", "ë¶ˆì•ˆ/ê±±ì •",
    "ê¸°ì¨", "ì•ˆì‹¬/ì‹ ë¢°"]

# âœ… ê°ì„± ë¶„ì„ ì‹¤í–‰
def run_sentiment_analysis():
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        forum_data = json.load(f)

    results = []
    for entry in tqdm(forum_data, desc="ğŸ”„ ê°ì„± ë¶„ì„ ì§„í–‰ ì¤‘", unit="ë¬¸ì¥"):
        company = entry["ê¸°ì—…ëª…"]
        text = entry["í…ìŠ¤íŠ¸"]

        encoding = tokenizer(text, truncation=True, max_length=512, return_tensors="pt").to("cuda")
        preds = pipe.tokenizer.decode(encoding["input_ids"][0])
        preds = pipe(preds)[0]

        top_2_preds = sorted(preds, key=lambda x: x["score"], reverse=True)[:2]
        mapped_preds = [{"ê°ì •": LABELS[int(p["label"].split("_")[-1])], "ì ìˆ˜": p["score"]} for p in top_2_preds]

        results.append({
            "ê¸°ì—…ëª…": company,
            "í…ìŠ¤íŠ¸": text,
            "ë‚ ì§œ": entry["ë‚ ì§œ"],
            "ì¡°íšŒ": entry["ì¡°íšŒ"],
            "ì¶”ì²œ": entry["ì¶”ì²œ"],
            "ê°ì„±ë¶„ì„ê²°ê³¼": mapped_preds
        })
    return results

# âœ… FE1: ê°ì„± ë¶„ì„ ë°ì´í„° ì •ì œ
def fe1_processing(results):
    seen_texts = set()
    unique_results = []
    for entry in results:
        if entry["í…ìŠ¤íŠ¸"] not in seen_texts:
            seen_texts.add(entry["í…ìŠ¤íŠ¸"])
            unique_results.append(entry)
    results = unique_results

    # âœ… ê°ì • í•„í„°ë§ ìˆœì„œëŒ€ë¡œ ì ìš©
    results = [entry for entry in results if not any(e["ê°ì •"] == "ì—†ìŒ" for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"])]
    for entry in results:
        entry["ê°ì„±ë¶„ì„ê²°ê³¼"] = [e for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"] if e["ê°ì •"] not in ["ì‹ ê¸°í•¨/ê´€ì‹¬", "ì•„ê»´ì£¼ëŠ”"]]
    results = [
        entry for entry in results
        if not ("ë¹„ì¥í•¨" in {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]} and 
                {"ì—†ìŒ", "ê¸°ëŒ€ê°"} & {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]})
    ]
    for entry in results:
        entry["ê°ì„±ë¶„ì„ê²°ê³¼"] = [e for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"] if e["ì ìˆ˜"] > 0.7]
    results = [entry for entry in results if not any(e["ê°ì •"] == "ìš°ì­ëŒ/ë¬´ì‹œí•¨" for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"])]
    results = [
        entry for entry in results 
        if not ({"ê¹¨ë‹¬ìŒ", "ë†€ëŒ"} <= {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]})
    ]
    results = [entry for entry in results if not any(e["ê°ì •"] == "ë¶ˆìŒí•¨/ì—°ë¯¼" for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"])]
    results = [entry for entry in results if not any(e["ê°ì •"] == "í™˜ì˜/í˜¸ì˜" for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"])]
    results = [
        entry for entry in results 
        if not ({"ì¦ì˜¤/í˜ì˜¤", "í•œì‹¬í•¨"} <= {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]})
    ]
    for entry in results:
        if len(entry["ê°ì„±ë¶„ì„ê²°ê³¼"]) == 1 and entry["ê°ì„±ë¶„ì„ê²°ê³¼"][0]["ê°ì •"] == "ê¹¨ë‹¬ìŒ":
            entry["ê°ì„±ë¶„ì„ê²°ê³¼"] = []
    for entry in results:
        entry["ê°ì„±ë¶„ì„ê²°ê³¼"] = [e for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"] if e["ê°ì •"] not in ["ì¡´ê²½", "ë†€ëŒ"]]
    results = [entry for entry in results if entry["ê°ì„±ë¶„ì„ê²°ê³¼"]]
    results = [
        entry for entry in results
        if not ("ë¹„ì¥í•¨" in {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]} and 
                {"ê¹¨ë‹¬ìŒ"} & {e["ê°ì •"] for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"]})
    ]
    for entry in results:
        entry["ê°ì„±ë¶„ì„ê²°ê³¼"] = [e for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"] if e["ê°ì •"] != "ë¹„ì¥í•¨"]
    
    results = [entry for entry in results if not any(e["ê°ì •"] == "ê³ ë§ˆì›€" for e in entry["ê°ì„±ë¶„ì„ê²°ê³¼"])]
    results = [entry for entry in results if entry["ê°ì„±ë¶„ì„ê²°ê³¼"]]
    
    return results

# âœ… FE2: ìµœì¢… ë§¤ë§¤ ê°ì • ì ìˆ˜ ê³„ì‚° ë° ë°ì´í„° ë³´ì •
def fe2_processing(results):
    with open(KIND_FILE, "r", encoding="utf-8") as f:
        kind_data = json.load(f)

    # âœ… ê¸°ì—…ë³„ ìƒì¥ì¼ ì •ë³´ ì €ì¥
    kind_dict = {entry["ê¸°ì—…ëª…"]: datetime.strptime(entry["ìƒì¥ì¼"], "%Y-%m-%d") for entry in kind_data}

    # âœ… ê°ì • ë¶„ë¥˜
    ë§¤ë„ê°ì • = {"ë¶ˆí‰/ë¶ˆë§Œ", "ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§", "ì–´ì´ì—†ìŒ", "ì˜ì‹¬/ë¶ˆì‹ ", "ë¶ˆì•ˆ/ê±±ì •", 
                "ì§œì¦", "í•œì‹¬í•¨", "í™”ë‚¨/ë¶„ë…¸", "ë‹¹í™©/ë‚œì²˜", "ìŠ¬í””", "ì¦ì˜¤/í˜ì˜¤", "í˜ë“¦/ì§€ì¹¨"}

    ë§¤ìˆ˜ê°ì • = {"ê¸°ëŒ€ê°", "ì¦ê±°ì›€/ì‹ ë‚¨", "ì•ˆì‹¬/ì‹ ë¢°", "ê°ë™/ê°íƒ„", "ê¸°ì¨", "ê¹¨ë‹¬ìŒ"}

    # âœ… ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    final_results = []
    ë§¤ë§¤ì ìˆ˜_ëª©ë¡ = []

    for entry in results:
        ê¸°ì—…ëª… = entry["ê¸°ì—…ëª…"]
        ê¸€ì‘ì„±ì¼ = datetime.strptime(entry["ë‚ ì§œ"], "%Y/%m/%d")  
        ì¶”ì²œìˆ˜ = int(entry["ì¶”ì²œ"])

        # âœ… ìƒì¥ì¼ê¹Œì§€ ë‚¨ì€ ì¼ì ê³„ì‚°
        if ê¸°ì—…ëª… in kind_dict:
            ìƒì¥ì¼ = kind_dict[ê¸°ì—…ëª…]
            ë‚¨ì€ì¼ìˆ˜ = (ìƒì¥ì¼ - ê¸€ì‘ì„±ì¼).days
            ìƒì¥ì¼ê°€ì¤‘ì¹˜ = 1 / (1 + max(0, ë‚¨ì€ì¼ìˆ˜))  
        else:
            continue  

        # âœ… ì¶”ì²œ ìˆ˜ ê°€ì¤‘ì¹˜ ì ìš©
        ì¶”ì²œê°€ì¤‘ì¹˜ = math.log(ì¶”ì²œìˆ˜ + 1) + 1  

        # âœ… ì´ˆê¸° ë§¤ë§¤ê°ì •ì ìˆ˜ ê³„ì‚°
        ê°ì„±ê²°ê³¼ = entry["ê°ì„±ë¶„ì„ê²°ê³¼"]
        ë§¤ë„ì ìˆ˜ = []
        ë§¤ìˆ˜ì ìˆ˜ = []

        for ê°ì • in ê°ì„±ê²°ê³¼:
            ê°ì •ëª… = ê°ì •["ê°ì •"]
            ì ìˆ˜ = ê°ì •["ì ìˆ˜"]

            if ê°ì •ëª… in ë§¤ë„ê°ì •:
                ë§¤ë„ì ìˆ˜.append(-ì ìˆ˜)  
            elif ê°ì •ëª… in ë§¤ìˆ˜ê°ì •:
                ë§¤ìˆ˜ì ìˆ˜.append(ì ìˆ˜)  

        if ë§¤ë„ì ìˆ˜ and ë§¤ìˆ˜ì ìˆ˜:
            ì´ˆê¸°ë§¤ë§¤ì ìˆ˜ = sum(ë§¤ìˆ˜ì ìˆ˜) + sum(ë§¤ë„ì ìˆ˜)
        elif ë§¤ë„ì ìˆ˜:
            ì´ˆê¸°ë§¤ë§¤ì ìˆ˜ = sum(ë§¤ë„ì ìˆ˜) / len(ë§¤ë„ì ìˆ˜)
        elif ë§¤ìˆ˜ì ìˆ˜:
            ì´ˆê¸°ë§¤ë§¤ì ìˆ˜ = sum(ë§¤ìˆ˜ì ìˆ˜) / len(ë§¤ìˆ˜ì ìˆ˜)
        else:
            ì´ˆê¸°ë§¤ë§¤ì ìˆ˜ = 0  

        # âœ… ìµœì¢… ë§¤ë§¤ê°ì •ì ìˆ˜ ê³„ì‚°
        ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜ = ì´ˆê¸°ë§¤ë§¤ì ìˆ˜ * ìƒì¥ì¼ê°€ì¤‘ì¹˜ * ì¶”ì²œê°€ì¤‘ì¹˜

        # âœ… ìµœì¢… ê²°ê³¼ ì €ì¥
        final_results.append({
            "ê¸°ì—…ëª…": ê¸°ì—…ëª…,
            "ìƒì¥ì¼": ìƒì¥ì¼.strftime("%Y-%m-%d"),
            "ë‚ ì§œ": entry["ë‚ ì§œ"],
            "ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜": round(ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜, 6)  
        })

        # ğŸ”¥ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
        ë§¤ë§¤ì ìˆ˜_ëª©ë¡.append(ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜)

    # âœ… í‰ê·  ë§¤ë§¤ê°ì •ì ìˆ˜ ê³„ì‚° (ê²°ì¸¡ê°’ ëŒ€ì²´ìš©)
    if ë§¤ë§¤ì ìˆ˜_ëª©ë¡:
        í‰ê· ë§¤ë§¤ì ìˆ˜ = sum(ë§¤ë§¤ì ìˆ˜_ëª©ë¡) / len(ë§¤ë§¤ì ìˆ˜_ëª©ë¡)
    else:
        í‰ê· ë§¤ë§¤ì ìˆ˜ = 0  

    print(f"âœ… í‰ê·  ë§¤ë§¤ê°ì •ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {round(í‰ê· ë§¤ë§¤ì ìˆ˜, 6)}")

    # âœ… KIND_38ì— ìˆëŠ” ê¸°ì—…ë“¤ì€ ë°˜ë“œì‹œ í¬í•¨ë˜ë„ë¡ ê²°ì¸¡ê°’ ì±„ìš°ê¸°
    existing_companies = {entry["ê¸°ì—…ëª…"] for entry in final_results}
    missing_companies = set(kind_dict.keys()) - existing_companies

    for ê¸°ì—…ëª… in missing_companies:
        ìƒì¥ì¼ = kind_dict[ê¸°ì—…ëª…]

        # âœ… 30ì¼ ì „ë¶€í„° 7ì¼ ê°„ê²©ìœ¼ë¡œ 4ê°œ ì—”íŠ¸ë¦¬ ì¶”ê°€
        start_date = ìƒì¥ì¼ - timedelta(days=30)
        for i in range(4):  
            ë‚ ì§œ = start_date + timedelta(days=i * 7)
            final_results.append({
                "ê¸°ì—…ëª…": ê¸°ì—…ëª…,
                "ìƒì¥ì¼": ìƒì¥ì¼.strftime("%Y-%m-%d"),
                "ë‚ ì§œ": ë‚ ì§œ.strftime("%Y/%m/%d"),
                "ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜": round(í‰ê· ë§¤ë§¤ì ìˆ˜, 6)  
            })

    print(f"âœ… KIND_38ì— ìˆëŠ” ê¸°ì—… ëª¨ë‘ í¬í•¨ ì™„ë£Œ! ì¶”ê°€ëœ ê¸°ì—… ê°œìˆ˜: {len(missing_companies)}")

    return final_results


def process_daily_average(final_results):
    company_date_scores = defaultdict(lambda: defaultdict(list))

    for entry in final_results:
        ê¸°ì—…ëª… = entry["ê¸°ì—…ëª…"]
        ìƒì¥ì¼ = datetime.strptime(entry["ìƒì¥ì¼"], "%Y-%m-%d")
        ë‚ ì§œ = datetime.strptime(entry["ë‚ ì§œ"], "%Y/%m/%d")
        ì ìˆ˜ = entry["ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜"]
        
        if (ìƒì¥ì¼ - ë‚ ì§œ).days <= 30:
            company_date_scores[ê¸°ì—…ëª…][ë‚ ì§œ].append(ì ìˆ˜)

    processed_results = []
    for ê¸°ì—…ëª…, date_scores in company_date_scores.items():
        ìƒì¥ì¼ = next(entry["ìƒì¥ì¼"] for entry in final_results if entry["ê¸°ì—…ëª…"] == ê¸°ì—…ëª…)

        for ë‚ ì§œ, scores in sorted(date_scores.items()):
            í‰ê· ì ìˆ˜ = sum(scores) / len(scores)
            processed_results.append({
                "ê¸°ì—…ëª…": ê¸°ì—…ëª…,
                "ìƒì¥ì¼": ìƒì¥ì¼,
                "ë‚ ì§œ": ë‚ ì§œ.strftime("%Y-%m-%d"),
                "ìµœì¢…ë§¤ë§¤ê°ì •ì ìˆ˜": round(í‰ê· ì ìˆ˜, 6)
            })
    
    return processed_results


# âœ… ì „ì²´ ì‹¤í–‰
if __name__ == "__main__":
    results = run_sentiment_analysis()
    results = fe1_processing(results)
    final_results = fe2_processing(results)
    
    # âœ… FE2 ì²˜ë¦¬ í›„ ê²°ê³¼ ì €ì¥ (ver1)
    with open(FE2_OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… FE2 ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {FE2_OUTPUT_FILE}")

    # âœ… FE2 ëë‚œ í›„ daily_average ì²˜ë¦¬ ì‹¤í–‰
    final_results_avg = process_daily_average(final_results)
    
    # âœ… ìµœì¢… ê²°ê³¼ ì €ì¥ (ver2)
    with open(FE2_DAILY_OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_results_avg, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… ì €ì¥ëœ íŒŒì¼: {FE2_DAILY_OUTPUT_FILE}")