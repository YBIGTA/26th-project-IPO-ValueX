# import os
# import json
# import re
# import pandas as pd

# from datetime import datetime
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# from time import sleep
# import time
# import random
# from abc import ABC
# from typing import List, Optional, Dict

# class RatioCrawler(ABC):
#     def __init__(self, output_dir):
#         self.output_dir=output_dir
#         self.base_url= "https://www.38.co.kr/html/fund/index.htm?o=r1"
#         self.driver: Optional[webdriver.Chrome] = None
#         self.company_data: List[Dict] = []
#         self.not_found: List[str]=[]
#         self.lost_list: List[str]=[]
        
#     def start_browser(self, json_file:str):
#         try:
#             self.company_data=json.load(open(json_file,"r",encoding="utf-8-sig"))
#             chrome_options=Options()
#             self.driver=webdriver.Chrome(options=chrome_options)
#             self.driver.get(self.base_url)
#             print("38 ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
#         except:
#             print("38 ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í˜ì´ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜")
#             raise

#     def search_company(self, company: str):
#         search_box=self.driver.find_element(By.ID, "string")
#         search_keyword=company
#         search_box.send_keys(search_keyword)

#         search_button=self.driver.find_element(By.XPATH, "/html/body/table[3]/tbody/tr/td/table[1]/tbody/tr/td[1]/form/table/tbody/tr/td[4]/input")
#         search_button.click()

#         WebDriverWait(self.driver,3).until(EC.presence_of_all_elements_located((By.TAG_NAME, "a")))

#         soup=BeautifulSoup(self.driver.page_source,"html.parser")

#         table=soup.find("table",attrs={"summary":"ìˆ˜ìš”ì˜ˆì¸¡ê²°ê³¼"})

#         try:
#             row=table.find("a", text=lambda text: text and company in text).find_parent("tr")
#         except:
#             print(f"{company} Not found.")
#             self.not_found.append(company)
#             return None
        
#         ratio=None
#         column=row.find_all("td")[6].get_text(strip=True)

#         if column=="-":
#             ratio="0 %"
#         else:
#             ratio=column
#         self.driver.get(self.base_url)
#         return ratio
    
#     def get_company_name(self):
#         companies=[]
#         for company in self.company_data:
#             name=list(company.keys())[0]
#             if company[name]["ìˆ˜ìš”ì˜ˆì¸¡"]["ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨"]=="":
#                 companies.append(name)
#         self.lost_list=companies
#         print(self.lost_list)
    
#     def crawl(self):
#         for lost_company in self.lost_list:
#             for company in self.company_data:
#                 company_name=list(company.keys())[0]
#                 if lost_company==company_name:
#                     try:
#                         ratio=self.search_company(company_name)
#                         company[company_name]["ìˆ˜ìš”ì˜ˆì¸¡"]["ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨"]=ratio
#                         print(f"{lost_company} ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ ìˆ˜ì •ì™„ë£Œ")
#                     except:
#                         print(f"{lost_company} ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
#                         self.not_found.append(lost_company)

#     def save(self):
#         with open("./Finance_data/IPOSTOCK_data.json", "w", encoding="utf-8-sig") as f:
#             json.dump(self.company_data, f, ensure_ascii=False, indent=4)

#         print("âœ… JSON íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

#         self.driver.quit()

# if __name__=="__main__":
#     output_directory="./Finance_data/"

#     crawler=RatioCrawler(output_dir=output_directory)
#     crawler.start_browser("./Finance_data/IPOSTOCK_data.json")
#     crawler.get_company_name()
#     crawler.crawl()
#     crawler.save()

import os
import json
import re
import pandas as pd

from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from time import sleep
import time
import random
from abc import ABC
from typing import List, Optional, Dict, Any

class RatioCrawler(ABC):
    def __init__(self, json_file):
        self.json_file = json_file
        self.base_url = "https://www.38.co.kr/html/fund/index.htm?o=r1"
        self.driver: Optional[webdriver.Chrome] = None
        self.company_data: List[Dict[str, Any]] = []  # JSON ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
        self.not_found: List[str] = []
        self.lost_list: List[str] = []

    def start_browser(self):
        """ Seleniumì„ ì´ìš©í•´ ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•˜ê³  38 ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í˜ì´ì§€ë¥¼ ë¡œë”© """
        try:
            with open(self.json_file, "r", encoding="utf-8-sig") as f:
                self.company_data = json.load(f)  # JSON ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œ
            
            chrome_options = Options()
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.get(self.base_url)
            print("âœ… 38 ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print("âŒ 38 ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í˜ì´ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜:", str(e))
            raise

    def search_company(self, company: str):
        """ íŠ¹ì • ê¸°ì—…ì˜ ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ì„ ì°¾ìŒ """
        try:
            # ê²€ìƒ‰ì°½ ì°¾ê¸°
            search_box = self.driver.find_element(By.ID, "string")
            search_box.clear()  # ì´ì „ ê²€ìƒ‰ ë‚´ìš© ì´ˆê¸°í™”
            search_box.send_keys(company)

            # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
            search_button = self.driver.find_element(By.XPATH, "/html/body/table[3]/tbody/tr/td/table[1]/tbody/tr/td[1]/form/table/tbody/tr/td[4]/input")
            search_button.click()

            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 3).until(EC.presence_of_all_elements_located((By.TAG_NAME, "a")))

            # í˜ì´ì§€ì—ì„œ ë°ì´í„° íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            table = soup.find("table", attrs={"summary": "ìˆ˜ìš”ì˜ˆì¸¡ê²°ê³¼"})

            # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            if not table:
                print(f"âŒ {company} - ìˆ˜ìš”ì˜ˆì¸¡ê²°ê³¼ í…Œì´ë¸” ì—†ìŒ")
                self.not_found.append(company)
                return None

            # ê¸°ì—…ëª…ì´ í¬í•¨ëœ <a> íƒœê·¸ ì°¾ê¸°
            link = table.find("a", text=lambda text: text and company in text)
            if not link:
                print(f"âŒ {company} - ê¸°ì—…ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                self.not_found.append(company)
                return None

            # í•´ë‹¹ <tr> ì°¾ê¸°
            row = link.find_parent("tr")
            if not row:
                print(f"âŒ {company} - í•´ë‹¹ í–‰(tr) ì—†ìŒ")
                self.not_found.append(company)
                return None

            # ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ ê°’ ê°€ì ¸ì˜¤ê¸°
            columns = row.find_all("td")
            if len(columns) < 7:
                print(f"âŒ {company} - ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ ë°ì´í„° ë¶€ì¡±")
                self.not_found.append(company)
                return None

            ratio = columns[6].get_text(strip=True)  # 7ë²ˆì§¸ ì»¬ëŸ¼ (index 6)
            if ratio == "-":
                ratio = "0 %"

            print(f"âœ… {company} - ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨: {ratio}")

            # ê²€ìƒ‰ í›„ ì›ë˜ í˜ì´ì§€ë¡œ ë³µê·€
            self.driver.get(self.base_url)
            return ratio
        except Exception as e:
            print(f"âŒ {company} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.not_found.append(company)
            return None

    def get_company_name(self):
        """ ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ì´ ë¹ˆ ê°’ì´ê±°ë‚˜ Noneì¸ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ìƒì„± """
        companies = []
        for company in self.company_data:
            company_name = list(company.keys())[0]  # ê¸°ì—…ëª… ê°€ì ¸ì˜¤ê¸°
            if not company[company_name]["ìˆ˜ìš”ì˜ˆì¸¡"].get("ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨"):  # ë¹ˆ ê°’ ë˜ëŠ” None ì²´í¬
                companies.append(company_name)
        self.lost_list = companies
        print(f"ğŸ” ê²€ìƒ‰í•´ì•¼ í•  ê¸°ì—… ìˆ˜: {len(self.lost_list)}")

    def crawl(self):
        """ ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ì´ ì—†ëŠ” ê¸°ì—…ì„ ê²€ìƒ‰í•˜ì—¬ ì—…ë°ì´íŠ¸ """
        for lost_company in self.lost_list:
            for company in self.company_data:
                company_name = list(company.keys())[0]
                if lost_company == company_name:
                    try:
                        ratio = self.search_company(company_name)
                        if ratio is not None:
                            company[company_name]["ìˆ˜ìš”ì˜ˆì¸¡"]["ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨"] = ratio
                            print(f"âœ… {lost_company} ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨ ìˆ˜ì • ì™„ë£Œ")
                        else:
                            print(f"âŒ {lost_company} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
                            self.not_found.append(lost_company)
                    except Exception as e:
                        print(f"âŒ {lost_company} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        self.not_found.append(lost_company)

    def save(self):
        """ JSON ë°ì´í„°ë¥¼ ì €ì¥ """
        with open(self.json_file, "w", encoding="utf-8-sig") as f:
            json.dump(self.company_data, f, ensure_ascii=False, indent=4)

        print("âœ… JSON íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

        self.driver.quit()


# ì‹¤í–‰
if __name__ == "__main__":
    json_file_path = "./Finance_data/IPOSTOCK_data.json"

    crawler = RatioCrawler(json_file=json_file_path)
    crawler.start_browser()
    crawler.get_company_name()
    crawler.crawl()
    crawler.save()
