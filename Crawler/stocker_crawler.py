import os
import json
import random
import pandas as pd
from datetime import datetime
from argparse import ArgumentParser
from typing import Dict, List
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from base_crawler import BaseCrawler
from utils.logger import setup_logger


class StockerCrawler(BaseCrawler):
    def __init__(self, output_dir: str, start_page: int = 1, end_page: int = 500, proxy: str = None):
        self.output_dir = output_dir
        self.start_page = start_page
        self.end_page = end_page
        self.base_url_template = "https://stocker.kr/stock_discuss/page/{}"
        self.driver = None
        self.reviews: List[Dict[str, str]] = []
        self.logger = setup_logger(log_file='./utils/stocker.log')
        self.proxy = proxy

    def start_browser(self):
        """âœ… í¬ë¡¬ ë¸Œë¼ìš°ì € ì‹¤í–‰"""
        self.logger.info("ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤â€¦")
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ğŸ”¹ í¬ë¡¬ ì°½ ìˆ¨ê¹€ (í•„ìš”í•˜ë©´ í•´ì œ)
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1400,600")
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"user-agent={user_agent}")

        if self.proxy:
            chrome_options.add_argument(f"--proxy-server=http://{self.proxy}")

        service = Service(ChromeDriverManager().install())
        self.browser = webdriver.Chrome(service=service, options=chrome_options)
        self.browser.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.browser.execute_cdp_cmd("Network.setUserAgentOverride", {"userAgent": user_agent})
        self.logger.info("ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„±ê³µ!")

    def scrape_reviews(self):
        """âœ… í¬ë¡¤ë§ ì‹¤í–‰"""
        self.logger.info("í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤â€¦")
        self.start_browser()

        for page_num in range(self.start_page, self.end_page + 1):
            page_url = self.base_url_template.format(page_num)
            self.logger.info(f"ğŸ“Œ {page_num}í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {page_url}")
            self.browser.get(page_url)
            WebDriverWait(self.browser, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

            try:
                BOX_PATH = "/html/body/div[4]/main/div[1]/div[2]/div/div[2]/div/ul"
                WebDriverWait(self.browser, 10).until(EC.visibility_of_element_located((By.XPATH, BOX_PATH)))

                review_elements = self.browser.find_elements(By.XPATH, '/html/body/div[4]/main/div[1]/div[2]/div/div[2]/div/ul/li')
                self.logger.info(f"ì´ {len(review_elements)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                for i, review in enumerate(review_elements):
                    li_class = review.get_attribute("class")
                    if li_class and "notice" in li_class:
                        self.logger.info(f"ê³µì§€ì‚¬í•­ ê¸€({i+1}ë²ˆì§¸ ê¸€) â†’ ê±´ë„ˆëœ€")
                        continue

                    try:
                        page_url = review.find_element(By.XPATH, f'{BOX_PATH}/li[{i+1}]/a').get_attribute("href")
                        self.browser.execute_script("window.open('');")
                        self.browser.switch_to.window(self.browser.window_handles[-1])
                        self.browser.get(page_url)

                        WebDriverWait(self.browser, 10).until(EC.presence_of_element_located((By.XPATH, '//h1')))
                        self.logger.info(f"{page_num}í˜ì´ì§€, {i+1}ë²ˆì§¸ ê¸€ í¬ë¡¤ë§ ì¤‘: {page_url}")

                        title = self.browser.find_element(By.XPATH, '//h1').text
                        date = self.browser.find_element(By.XPATH, '//div[@class="app-article-meta"]/el-tooltip').get_attribute("content")
                        views = self.browser.find_element(By.XPATH, '//div[@class="app-article-meta"]/div[2]').text
                        likes = self.browser.find_element(By.XPATH, '//span[@id="vm_v_count"]').text
                        unlikes = self.browser.find_element(By.XPATH, '//span[@id="vm_d_count"]').text

                        try:
                            contents = self.browser.find_elements(By.XPATH, '//div[@class="app-article-content app-clearfix"]/div[2]/*')
                            content_list = [content.get_attribute("innerText").strip() for content in contents]
                        except:
                            content_list = []

                        try:
                            comments = self.browser.find_elements(By.XPATH, '//ul[@id="capp-board-comment-list"]/li//p')
                            comment_list = [comment.text.strip() for comment in comments if comment.text.strip()]
                        except:
                            comment_list = []

                        self.reviews.append({
                            "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€": page_num,
                            "title": title,
                            "ì‘ì„±ì‹œê°„": date,
                            "ì¡°íšŒìˆ˜": views,
                            "ì¶”ì²œìˆ˜": likes,
                            "ë¹„ì¶”ì²œìˆ˜": unlikes,
                            "ë§í¬": page_url,
                            "ë³¸ë¬¸": json.dumps(content_list, ensure_ascii=False),
                            "ëŒ“ê¸€": json.dumps(comment_list, ensure_ascii=False)
                        })

                        self.browser.close()
                        self.browser.switch_to.window(self.browser.window_handles[0])

                    except Exception as e:
                        self.logger.error(f"ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            except Exception as e:
                self.logger.error(f"ë¦¬ë·° ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨: {e}")

        self.logger.info("í¬ë¡¤ë§ ì™„ë£Œ. ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        self.browser.quit()
        self.save_to_database()

    def save_to_database(self):
        """âœ… ë°ì´í„° ì €ì¥ (í•œ íŒŒì¼ì— ëˆ„ì )"""
        if not self.reviews:
            self.logger.info("ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        output_path = os.path.join(self.output_dir, "stocker_pages_final.csv")
        df = pd.DataFrame(self.reviews, columns=[
            "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€", "title", "ì‘ì„±ì‹œê°„", "ì¡°íšŒìˆ˜", "ì¶”ì²œìˆ˜", "ë¹„ì¶”ì²œìˆ˜", "ë§í¬", "ë³¸ë¬¸", "ëŒ“ê¸€"
        ])

        file_exists = os.path.exists(output_path)
        mode = "a" if file_exists else "w"
        header = not file_exists

        df.to_csv(output_path, index=False, encoding="utf-8-sig", mode=mode, header=header)
        self.logger.info(f"ğŸ“Œ í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
        self.reviews.clear()


# âœ… ì‹¤í–‰ë¶€
if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument('-o', '--output_dir', type=str, required=True, help="Output file directory.")
    parser.add_argument('-s', '--start_page', type=int, default=1, help="Start page for crawling.")
    parser.add_argument('-e', '--end_page', type=int, default=500, help="End page for crawling.")
    args = parser.parse_args()

    crawler = StockerCrawler(output_dir=args.output_dir, start_page=args.start_page, end_page=args.end_page)
    crawler.scrape_reviews()