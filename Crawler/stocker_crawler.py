from crawling.base_crawler import BaseCrawler
from crawling.utils.logger import setup_logger

from selenium.webdriver.chrome.webdriver import WebDriver

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys

from typing import Dict, List
import pandas as pd
import time
import os
import sys
import json

import random
from datetime import datetime

from image_ocr import process_image_ocr

#-----------------------------------------
config = ('-l  kor+kor_vert+eng')

proxy_list = [
    "15.236.203.245:3128",
    "98.80.66.1:10018",
    "44.219.175.186:80"
]
#-----------------------------------------

class StockerCrawler(BaseCrawler):
    def __init__(self, output_dir: str, start_page: int = 1, end_page: int = 43, proxy: str = None):
        self.output_dir = output_dir
        self.start_page = start_page
        self.end_page = end_page  # 500í˜ì´ì§€ê¹Œì§€ ìë™ í¬ë¡¤ë§
        # ê°œë…ê¸€, ë¦¬ìŠ¤íŠ¸ 100ê°œì”©ìœ¼ë¡œ ì§„í–‰
        self.base_url_template = "https://stocker.kr/stock_discuss/page/{}"
        self.driver = None
        self.reviews: List[Dict[str, str]] = []
        self.logger = setup_logger(log_file='./crawling/utils/stalker.log')  # Logger ì„¤ì •
        self.proxy = None #random.choice(proxy_list) 


    def start_browser(self):
        """
        í¬ë¡¬ ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
        """
        self.logger.info("ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤â€¦")
        chrome_options = Options()
        chrome_options.add_argument('â€”disable-blink-features=AutomationControlled')
        # chrome_options.add_argument('â€”start-maximized')  # ë¸Œë¼ìš°ì € ìµœëŒ€í™”ë¡œ ì‹œì‘
        chrome_options.add_argument("--headless")  # ğŸ”¹ ê¸°ë³¸ì ìœ¼ë¡œ í¬ë¡¬ ì°½ì„ ìˆ¨ê¹€
        chrome_options.add_argument("--disable-gpu")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ GPU ë Œë”ë§ ë°©ì§€
        chrome_options.add_argument("--window-size=1400,600")  # í¬ê¸° ì„¤ì •
        # chrome_options.add_argument("--incognito") #ì‹œí¬ë¦¿ ëª¨ë“œ : ì¿ í‚¤ ì—†ìŒ

        # ë¸Œë¼ìš°ì € User-Agent ì„¤ì • (ì‹¤ì œ ë¸Œë¼ìš°ì € User-Agentë¡œ ë³€ê²½)
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"user-agent={user_agent}")

        #í”„ë¡ì‹œ ì‚¬ìš©
        if self.proxy:
            chrome_options.add_argument(f"--proxy-server=http://{self.proxy}")

        service = Service(ChromeDriverManager().install())  # ChromeDriver ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ì‹¤í–‰
        self.browser = webdriver.Chrome(service=service, options=chrome_options)

        # âœ… navigator.webdriver ì†ì„± ìš°íšŒ (ìë™í™” íƒì§€ ì°¨ë‹¨)
        self.browser.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        # âœ… CDP(Chrome DevTools Protocol)ì—ì„œ User-Agent ë³€ê²½ ì ìš©
        self.browser.execute_cdp_cmd("Network.setUserAgentOverride", {"userAgent": user_agent})

        self.logger.info("ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„±ê³µ!")

    def scrape_reviews(self):
        self.logger.info("í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤â€¦")
        self.start_browser()

        for page_num in range(self.start_page, self.end_page + 1):
            self.reviews.clear()  # ë§¤ í˜ì´ì§€ë§ˆë‹¤ reviews ì´ˆê¸°í™”
            page_url = self.base_url_template.format(page_num)
            self.logger.info(f"ğŸ“Œ {page_num}í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {page_url}")

            # Step 1: í˜ì´ì§€ ì ‘ì†
            # ğŸ”¹ í˜ì´ì§€ê°€ ë³€ê²½ë  ë•Œë§Œ ë¸Œë¼ìš°ì €ë¥¼ ë³´ì´ê²Œ ì„¤ì •
            self.browser.set_window_position(0, 0)  # í™”ë©´ì— ë³´ì´ë„ë¡ ë³€ê²½
            self.browser.get(page_url)
            time.sleep(1)  # í˜ì´ì§€ê°€ ë³´ì¼ ì‹œê°„ì„ ì•½ê°„ ì¤Œ
            self.browser.minimize_window()  # ğŸ”¹ ì°½ì„ ìµœì†Œí™”í•˜ì—¬ ìˆ¨ê¹€ 
            self.logger.info(f"URL ì—´ê¸° ì„±ê³µ: {page_url}")
            time.sleep(random.uniform(2, 5))  # ëœë¤ ë”œë ˆì´ ì ìš© (ì„œë²„ ë¶€í•˜ ë°©ì§€)


        # Step 2: ëª©ë¡ì—ì„œ ìš”ì†Œ ë°›ì•„ì˜¤ê¸° & ê¸€ ê±°ë¥´ê¸°(ìš°ì„  ë‚˜ì¤‘ì— ì²˜ë¦¬)
            try:
                BOX_PATH = "/html/body/div[4]/main/div[1]/div[2]/div/div[2]/div/ul"
                #self.browser.find_element(By.CLASS_NAME, "app-board-template-list")
                WebDriverWait(self.browser, 10).until(EC.visibility_of_element_located((By.XPATH, BOX_PATH)))

                review_elements = self.browser.find_elements(By.XPATH, '/html/body/div[4]/main/div[1]/div[2]/div/div[2]/div/ul/li')
                self.logger.info(f"ì´ {len(review_elements)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                for i, review in enumerate(review_elements):
                    time.sleep(random.uniform(5, 10))  # âœ… Crawl-delay ì¤€ìˆ˜ (ëœë¤ ë”œë ˆì´ ì ìš©)

                    # âœ… í´ë˜ìŠ¤ëª…ì´ 'notice'ì´ë©´ ê±´ë„ˆë›°ê¸°
                    li_class = review.get_attribute("class")
                    if li_class and "notice" in li_class:
                        self.logger.info(f"ê³µì§€ì‚¬í•­ ê¸€({i+1}ë²ˆì§¸ ê¸€) â†’ ê±´ë„ˆëœ€")
                        continue

        # Step 3: ë³¸ë¬¸ ì•ˆìœ¼ë¡œ ì…ì¥
                    try:
                        page_url = review.find_element(By.XPATH, f'{BOX_PATH}/li[{i+1}]/a').get_attribute("href")
                        # self.logger.info(f"{page_url}")
                        self.browser.execute_script("window.open('');")  # ìƒˆ íƒ­ ì—´ê¸°
                        self.browser.switch_to.window(self.browser.window_handles[-1])
                        self.browser.get(page_url)  # í•´ë‹¹ URLë¡œ ì´ë™
                        # time.sleep(1)
                        self.logger.info(f"{page_num}ë²ˆì§¸ í˜ì´ì§€, {i+1}ë²ˆì§¸ ê¸€ë¡œ ì…ì¥í•©ë‹ˆë‹¤.")


        # Step 4: ë³¸ë¬¸ ì•ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        try:
                            WebDriverWait(self.browser, 10).until(EC.presence_of_element_located((By.XPATH, '//h1')))#[@class="tw-font-bold tw-text-3xl md:tw-text-xl"]')))
                            self.logger.info("ê¸€ì— ì…ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
                            title = self.browser.find_element(By.XPATH, '//h1').text
                            # crawling_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            date = self.browser.find_element(By.XPATH, '//div[@class="app-article-meta"]/el-tooltip').get_attribute("content")
                            views = self.browser.find_element(By.XPATH, '//div[@class="app-article-meta"]/div[2]').text
                            likes = self.browser.find_element(By.XPATH, '//span[@id="vm_v_count"]').text
                            unlikes = self.browser.find_element(By.XPATH, '//span[@id="vm_d_count"]').text

                            self.logger.info("ê¸€ë“¤ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

                            try:
                                contents = self.browser.find_elements(By.XPATH, '//div[@class="app-article-content app-clearfix"]/div[2]/*')
                                self.logger.info(f"ë³¸ë¬¸ ì•ˆì—ì„œ ê¸€ ê°œìˆ˜: {len(contents)}ê°œ")


                                content_list = []
                                for l, content in enumerate(contents):
                                    content_list.append(content.get_attribute("innerText").strip()) 
                    
                                self.logger.info(f"ì¶”ì¶œëœ ë³¸ë¬¸ ê°œìˆ˜: {len(content_list)}")

                            except Exception as e:
                                self.logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ error ë°œìƒ: {e}")

        # Step 5: ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
                            try:
                                comments = self.browser.find_elements(By.XPATH, '//ul[@id="capp-board-comment-list"]/li//p')
                                comment_list = []
                                for l, comment in enumerate(comments):
                                    # ì ìš©ì´ ì•ˆë¨ ã… 
                                    # if comment is None or comment.text is None or comment.text.strip() == "":
                                    #     continue  # ë¹ˆ ëŒ“ê¸€ì´ë©´ ê±´ë„ˆëœ€
                                    comment_list.append(comment.text.strip()) 
                
                                self.logger.info(f"ì¶”ì¶œëœ ëŒ“ê¸€ ê°œìˆ˜: {len(comment_list)}")
                            except Exception as e:
                                self.logger.error(f"ëŒ“ê¸€ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

                            self.reviews.append({
                                "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€": page_num,
                                # "í¬ë¡¤ë§ ì‹œê°„": crawling_time,
                                "title": title,
                                "ì‘ì„±ì‹œê°„" : date,
                                "ì¡°íšŒìˆ˜": views,
                                "ì¶”ì²œìˆ˜": likes,
                                "ë¹„ì¶”ì²œìˆ˜": unlikes,
                                "ë§í¬" : page_url,
                                "ë³¸ë¬¸" : json.dumps(content_list, ensure_ascii=False),
                                "ëŒ“ê¸€" : comment_list
                            })

                        except Exception as e:
                            self.logger.error(f"ë³¸ë¬¸ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

        
        # Step 7: í˜ì´ì§€ ë‹«ê³  ë³µê·€(ë‹¤ìŒ í˜ì´ì§€ ì¤€ë¹„)
                        self.browser.close()
                        self.logger.info("ê¸€ì„ ë‹«ê³  ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
                        self.logger.info("--------------------------------------------------------")
                        # time.sleep(1)
                        self.browser.switch_to.window(self.browser.window_handles[0])


                    except Exception as e:
                        self.logger.error(f"ë³¸ë¬¸ì— ì…ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

                    # âœ… 10ê°œ ì´ìƒì´ë©´ ì €ì¥ ì‹¤í–‰ (for-loop ë‚´ë¶€ì—ì„œ ì‹¤í–‰)
                    if len(self.reviews) >= 15:
                        self.logger.info("ğŸ“Œ 15ê°œ ë‹¨ìœ„ë¡œ ì €ì¥ ì‹¤í–‰")
                        self.save_to_database(page_num)

                # âœ… for-loop ì¢…ë£Œ í›„, ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì €ì¥
                if self.reviews:  
                    self.logger.info("ğŸ“Œ ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì €ì¥ ì‹¤í–‰")
                    self.save_to_database(page_num)

            except Exception as e:
                self.logger.error(f"ë¦¬ë·° ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

        # Step 7: ë¸Œë¼ìš°ì € ì¢…ë£Œ
        self.logger.info("í¬ë¡¤ë§ ì™„ë£Œ. ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        self.browser.quit()

    def save_to_database(self, page_num):
        """
        âœ… 10ê°œ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§í•œ ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        """
        if not self.reviews:
            self.logger.info("ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        output_path = os.path.join(self.output_dir, f"stocker_pages_{page_num}page.csv")

        # âœ… DataFrame ë³€í™˜ (ì»¬ëŸ¼ ìˆœì„œ ì§€ì •)
        df = pd.DataFrame(self.reviews, columns=[
            "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€", "title", "ì‘ì„±ì‹œê°„", "ì¡°íšŒìˆ˜", "ì¶”ì²œìˆ˜", "ë¹„ì¶”ì²œìˆ˜",
            "ë§í¬", "ë³¸ë¬¸", "ëŒ“ê¸€"
        ])

        # âœ… CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì €ì¥ ë°©ì‹ ê²°ì •
        file_exists = os.path.exists(output_path)

        if file_exists:
            # âœ… ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ append (header ì—†ì´ ì¶”ê°€)
            df.to_csv(output_path, index=False, encoding="utf-8-sig", mode="a", header=False)
            self.logger.info(f"ğŸ“Œ ê¸°ì¡´ CSV íŒŒì¼ì— 15ê°œ ì¶”ê°€ ì €ì¥: {output_path}")
        else:
            # âœ… ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ íŒŒì¼ ìƒì„± (header í¬í•¨)
            df.to_csv(output_path, index=False, encoding="utf-8-sig", mode="w", header=True)
            self.logger.info(f"ğŸ“Œ ìƒˆ CSV íŒŒì¼ ìƒì„± ë° ì €ì¥: {output_path}")

        # âœ… ì €ì¥ í›„ self.reviews ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.reviews.clear()