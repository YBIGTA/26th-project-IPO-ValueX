import os
import json


from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from time import sleep
import time
from abc import ABC
from typing import List, Optional, Dict

def load_company_names(json_file):
    with open(json_file,"r",encoding="utf-8") as f:
        data=json.load(f)
    return [company["ê¸°ì—…ëª…"] for company in data]

class IpostockCrawler(ABC):
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.base_url: str = 'http://www.ipostock.co.kr/sub03/ipo08.asp?str4=2024&str5=all'
        self.datas: List[Dict] = []
        self.driver: Optional[webdriver.Chrome] = None
        self.company_list:List[str] = load_company_names("Finance_data/KIND_data.json")

    def start_browser(self):
        try:
            chrome_options=Options()
            self.driver=webdriver.Chrome(options=chrome_options)
            self.driver.get(self.base_url)
            self.driver.implicitly_wait(20)
            print("ipostock í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print("ipostock í˜ì´ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜")
            raise

    def search_company(self, company:str):
        search_box = self.driver.find_element(By.CLASS_NAME, "FORM1")  # ê²€ìƒ‰ì°½ (name="str3")
        search_keyword = company
        search_box.send_keys(search_keyword)

        search_button=self.driver.find_element(By.XPATH, "//input[@type='image' and contains(@src, 'btn_search.gif')]")
        search_button.click()
        time.sleep(3)

        soup=BeautifulSoup(self.driver.page_source,"html.parser")

        for a in soup.find_all("a"):
            found_text=a.find("font").get_text(strip=True).rstrip(".") if a.find("font") else a.get_text(strip=True).rstrip(".")

            if not found_text:
                continue
            if company.startswith(found_text):
                company_url=a.get("href")
                full_url=f"http://www.ipostock.co.kr{company_url}"

                self.driver.get(full_url)
                time.sleep(2)
                print(f"\nğŸ” {company} ê²€ìƒ‰ ì™„ë£Œ\n")
                return

        

        # try:
        #     result = self.driver.find_element(By.XPATH, f"//a[contains(text(), '{company}')]")
        #     print("ê²€ìƒ‰ê²°ê³¼ ì¡´ì¬")
        #     result.click()
        #     print(f"{company} ê²€ìƒ‰ í›„ í´ë¦­ ì™„ë£Œ.")
            
        # except Exception as e:
        #     print(f"{company}ê²€ìƒ‰ê³¼ì •ì—ì„œ ì˜¤ë¥˜")
        #     raise
    

    def scrape_data(self):
        """ ê¸°ì—…ë³„ ë°ì´í„° í¬ë¡¤ë§ """
        result_data = []

        for company in self.company_list:
            self.search_company(company)

            time.sleep(2)
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            def get_data(label):
                """ íŠ¹ì • ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ """
                for td in soup.find_all("td"):
                    if td.get_text(strip=True) == label:  # íƒœê·¸ ë‚´ë¶€ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ ë¹„êµ
                        next_td = td.find_next_sibling("td")
                        return next_td.get_text(strip=True) if next_td else None
                return None
            
            # ìˆ˜ìš”ì˜ˆì¸¡ íƒ­ í¬ë¡¤ë§
            wanted_ipo_price = get_data("(í¬ë§)ê³µëª¨ê°€ê²©")
            competition_rate = get_data("ë‹¨ìˆœ ê¸°ê´€ê²½ìŸë¥ ")
            lockup_ratio = get_data("ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨")
            print(f"âœ… {company} ìˆ˜ìš”ì˜ˆì¸¡ tab í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"   â”œâ”€ (í¬ë§) ê³µëª¨ê°€ê²©: {wanted_ipo_price}")
            print(f"   â”œâ”€ ë‹¨ìˆœ ê¸°ê´€ê²½ìŸë¥ : {competition_rate}")
            print(f"   â””â”€ ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨: {lockup_ratio}\n")

            # ê³µëª¨ì •ë³´ íƒ­ìœ¼ë¡œ ì´ë™
            try:
                offering_info_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table/tbody/tr[5]/td/table[1]/tbody/tr[1]/td[4]/a")
                offering_info_btn.click()
                time.sleep(2)
                print(f"âœ… {company} ê³µëª¨ì •ë³´ tab í´ë¦­ ì™„ë£Œ")
                soup = BeautifulSoup(self.driver.page_source, "html.parser")

                confirmed_ipo_price = get_data("(í™•ì •)ê³µëª¨ê°€ê²©")
                subscription_rate = get_data("ì²­ì•½ê²½ìŸë¥ ")
                forecast_date = get_data("ìˆ˜ìš”ì˜ˆì¸¡ì¼")
                listing_date = get_data("ìƒì¥ì¼")
                print(f"âœ… {company} ê³µëª¨ì •ë³´ tab í¬ë¡¤ë§ ì™„ë£Œ")
                print(f"   â”œâ”€ (í™•ì •) ê³µëª¨ê°€ê²©: {confirmed_ipo_price}")
                print(f"   â”œâ”€ ì²­ì•½ê²½ìŸë¥ : {subscription_rate}")
                print(f"   â”œâ”€ ìˆ˜ìš”ì˜ˆì¸¡ì¼: {forecast_date}")
                print(f"   â””â”€ ìƒì¥ì¼: {listing_date}\n")
            except Exception as e:
                print(f"âš  {company} ê³µëª¨ì •ë³´ íƒ­ ì ‘ê·¼ ì˜¤ë¥˜: {e}\n")

            # ì£¼ì£¼êµ¬ì„± íƒ­ìœ¼ë¡œ ì´ë™
            try:
                stockholder_info_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table/tbody/tr[5]/td/table/tbody/tr[1]/td[2]/a")
                stockholder_info_btn.click()
                time.sleep(2)
                print(f"âœ… {company} ì£¼ì£¼êµ¬ì„± tab í´ë¦­ ì™„ë£Œ")
                soup = BeautifulSoup(self.driver.page_source, "html.parser")
            except Exception as e:
                print(f"âš  {company} ì£¼ì£¼êµ¬ì„± íƒ­ ì ‘ê·¼ ì˜¤ë¥˜: {e}\n")

            # ê³µëª¨ í›„ ë°œí–‰ì£¼ì‹ìˆ˜ í¬ë¡¤ë§
            issued_shares = None
            try:
                public_after_td = soup.find("td", string="ê³µëª¨í›„")
                if public_after_td:
                    parent_tr = public_after_td.find_parent("tr")
                    next_tr = parent_tr.find_next_sibling("tr")
                    if next_tr:
                        issued_shares_td = next_tr.find("td", string="ë°œí–‰ì£¼ì‹ìˆ˜")
                        if issued_shares_td:
                            issued_shares = issued_shares_td.find_next_sibling("td").text.strip()
                print(f"âœ… {company} ì£¼ì£¼êµ¬ì„± tab í¬ë¡¤ë§ ì™„ë£Œ")
                print(f"   â””â”€ ê³µëª¨ í›„ ë°œí–‰ì£¼ì‹ìˆ˜: {issued_shares}\n")
            except Exception as e:
                print(f"âš  {company}ì˜ ê³µëª¨ í›„ ë°œí–‰ì£¼ì‹ìˆ˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}\n")

            # JSON ë°ì´í„° ì €ì¥
            company_data = {
                company: {
                    "ìƒì¥ì¼": listing_date,
                    "ìˆ˜ìš”ì˜ˆì¸¡": {
                        "(í¬ë§)ê³µëª¨ê°€ê²©": wanted_ipo_price,
                        "ë‹¨ìˆœê¸°ê´€ê²½ìŸë¥ ": competition_rate,
                        "ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨": lockup_ratio
                    },
                    "ê³µëª¨ì •ë³´": {
                        "(í™•ì •)ê³µëª¨ê°€ê²©": confirmed_ipo_price,
                        "ì²­ì•½ê²½ìŸë¥ ": subscription_rate,
                        "ìˆ˜ìš”ì˜ˆì¸¡ì¼": forecast_date,
                        "ìƒì¥ì¼": listing_date,
                        "ê³µëª¨í›„ ìƒì¥ì£¼ì‹ìˆ˜": issued_shares
                    }
                }
            }

            result_data.append(company_data)
            self.driver.get(self.base_url)
            time.sleep(2)

        return result_data

    
    def save_to_database(self, data):
        if not data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # NEWBIE_PROJECT ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        base_dir = os.path.dirname(os.path.dirname(__file__))
        output_file = os.path.join(base_dir, "Finance_data", "IPOSTOCK_data.json")

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ìƒˆë¡œìš´ ë°ì´í„°ì™€ ë³‘í•©
        if os.path.exists(output_file):
            with open(output_file, mode='r', encoding='utf-8') as file:
                existing_data = json.load(file)
            print("ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        else:
            existing_data = []

        # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
        combined_data = existing_data + data

        # JSON íŒŒì¼ì— ë³‘í•©ëœ ë°ì´í„° ì €ì¥
        with open(output_file, mode='w', encoding='utf-8') as file:
            json.dump(combined_data, file, ensure_ascii=False, indent=4)

        print(f"ë°ì´í„°ê°€ {output_file}ì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def run(self):
        try:
            self.start_browser()

            # # ë‚ ì§œ ë²”ìœ„ ì„¤ì • ë° ì‚¬ì´íŠ¸ ì ‘ì†
            # self.select_date_range(start_date, end_date)

            # ë°ì´í„° í¬ë¡¤ë§
            data = self.scrape_data()

            # ë°ì´í„° ì €ì¥
            self.save_to_database(data)
        finally:
            if self.driver:
                self.driver.quit()
                print("ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    output_directory = os.path.join(os.path.dirname(__file__), "output")

    os.makedirs(output_directory, exist_ok=True)

    # # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    # start_date = "20140204"  # ì›í•˜ëŠ” ì‹œì‘ ë‚ ì§œ
    # end_date = "20160204"    # ì›í•˜ëŠ” ì¢…ë£Œ ë‚ ì§œ

    # # í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì§€ì •
    # max_pages_to_crawl = 10  # ì›í•˜ëŠ” í˜ì´ì§€ ìˆ˜ë¡œ ì„¤ì •

    crawler = IpostockCrawler(output_dir=output_directory)
    crawler.run()
