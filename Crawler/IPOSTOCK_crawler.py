import os
import json
import re
import pandas as pd

from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from time import sleep
import time
import random
from abc import ABC
from typing import List, Optional, Dict

def load_company_data(json_file):
    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)
        company_data = {}
        for company in data:
            company_data[company["ê¸°ì—…ëª…"]] = company["ìƒì¥ì¼"]
    return company_data

class IpostockCrawler(ABC):
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.base_url = "http://www.ipostock.co.kr/sub03/ipo08.asp?str4=2025&str5=all"  # 2025ë…„ ì „ì²´ë³´ê¸°
        self.result: List[Dict] = []
        self.driver: Optional[webdriver.Chrome] = None
        self.company_data: Dict = {}
        self.search_fail_list: List[str] = []
        self.spac_reits: List[str] = []

    def start_browser(self,json_file:str):
        try:
            self.company_data=load_company_data(json_file)
            chrome_options = Options()
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36")
            chrome_options.add_argument("accept-language=ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7")
            chrome_options.add_argument("accept-encoding=gzip, deflate, br")
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.get(self.base_url)
            print("ipostock í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print("ipostock í˜ì´ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜")
            raise

    def random_sleep(self, base=2, jitter=3):
        """ëœë¤í•œ ëŒ€ê¸° ì‹œê°„ì„ ì¶”ê°€í•˜ì—¬ ìš”ì²­ ì†ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤."""
        sleep_time = base + random.uniform(0, jitter)
        print(f"â³ ëŒ€ê¸° ì¤‘... {round(sleep_time, 2)}ì´ˆ")
        time.sleep(sleep_time)


    def search_company(self, company: str):

        search_box = self.driver.find_element(By.CLASS_NAME, "FORM1")  # ê²€ìƒ‰ì°½ (name="str3")
        search_keyword = company
        search_box.send_keys(search_keyword)

        search_button = self.driver.find_element(By.XPATH, "//input[@type='image' and contains(@src, 'btn_search.gif')]")
        search_button.click()

        # self.random_sleep()

        # time.sleep(3) ëŒ€ì‹ : ìµœì†Œí•œ <a> íƒœê·¸ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(self.driver, 3).until(EC.presence_of_all_elements_located((By.TAG_NAME, "a")))

        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        for a in soup.find_all("a"):
            found_text = a.find("font").get_text(strip=True).rstrip(".") if a.find("font") else a.get_text(strip=True).rstrip(".")
            if not found_text:
                continue
            if not company.lower().startswith(found_text.lower()):
                continue
            if company.lower().startswith(found_text.lower()):
                company_url = a.get("href")
                full_url = f"http://www.ipostock.co.kr{company_url}"
                try:
                    self.driver.get(full_url)

                    # self.random_sleep()

                    # time.sleep(2) ëŒ€ì‹ : í˜ì´ì§€ ë‚´ì— "(í¬ë§)ê³µëª¨ê°€ê²©" í…ìŠ¤íŠ¸ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
                    WebDriverWait(self.driver, 3).until(EC.presence_of_element_located(
                        (By.XPATH, "//*[contains(text(), '(í¬ë§)ê³µëª¨ê°€ê²©')]")
                    ))
                except Exception as e:
                    print(f"âš  {company}ì˜ full_url ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {full_url}. í•´ë‹¹ ê¸°ì—…ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n")
                    self.search_fail_list.append(company)
                    return False
                print(f"\nğŸ” {company} ê²€ìƒ‰ ì™„ë£Œ\n")
                return True
        print(f"âš  {company} ê²€ìƒ‰ ì‹¤íŒ¨! (ipostockì˜ ì´ë¦„ê³¼ ë‹¤ë¥¼ì§€ë„)\n")
        self.search_fail_list.append(company)
        return False

    def crawl(self, company: str):
        search_succes = self.search_company(company)

        # time.sleep(2) ëŒ€ì‹ : í˜ì´ì§€ ë‚´ì— ìµœì†Œí•œ í•˜ë‚˜ì˜ <td> íƒœê·¸ê°€ ì¡´ì¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.TAG_NAME, "td")))
        if not search_succes:
            print(f"âš  {company} ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì €ì¥\n")
            company_data = {
                company: {
                    "ìˆ˜ìš”ì˜ˆì¸¡": {
                        "(í¬ë§)ê³µëª¨ê°€ê²©": None,
                        "ë‹¨ìˆœê¸°ê´€ê²½ìŸë¥ ": None,
                        "ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨": None
                    },
                    "ê³µëª¨ì •ë³´": {
                        "(í™•ì •)ê³µëª¨ê°€ê²©": None,
                        "ì²­ì•½ê²½ìŸë¥ ": None,
                        "ìˆ˜ìš”ì˜ˆì¸¡ì¼": None,
                        "ìƒì¥ì¼": None,
                    },
                    "ì£¼ì£¼êµ¬ì„±": {
                        "ê³µëª¨í›„ ë°œí–‰ì£¼ì‹ìˆ˜": None,
                        "ì£¼ì£¼êµ¬ì„± table": None
                    },
                    "ì¬ë¬´ì •ë³´": None,
                    "ì¢…ê°€ëŒ€ë¹„ë“±ë½ìœ¨": None
                }
            }
            self.result.append(company_data)
            return

        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        def get_data(label):
            """ íŠ¹ì • ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ """
            for td in soup.find_all("td"):
                if td.get_text(strip=True) == label:  # íƒœê·¸ ë‚´ë¶€ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ ë¹„êµ
                    next_td = td.find_next_sibling("td")
                    return next_td.get_text(strip=True) if next_td else None
            return None

        # ìˆ˜ìš”ì˜ˆì¸¡ íƒ­ í¬ë¡¤ë§
        wanted_ipo_price = get_data("(í¬ë§)ê³µëª¨ê°€ê²©")
        competition_rate = get_data("ë‹¨ìˆœ ê¸°ê´€ê²½ìŸë¥ ")
        lockup_ratio = get_data("ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨")
        print(f"âœ… {company} ìˆ˜ìš”ì˜ˆì¸¡ tab í¬ë¡¤ë§ ì™„ë£Œ")

        # ê³µëª¨ì •ë³´ íƒ­ìœ¼ë¡œ ì´ë™
        try:
            offering_info_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table/tbody/tr[5]/td/table[1]/tbody/tr[1]/td[4]/a")
            offering_info_btn.click()
            # time.sleep(2) ëŒ€ì‹ : í˜ì´ì§€ ë‚´ì— "(í™•ì •)ê³µëª¨ê°€ê²©" í…ìŠ¤íŠ¸ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            WebDriverWait(self.driver, 3).until(EC.presence_of_element_located(
                (By.XPATH, "//*[contains(text(), '(í™•ì •)ê³µëª¨ê°€ê²©')]")
            ))
            print(f"âœ… {company} ê³µëª¨ì •ë³´ tab í´ë¦­ ì™„ë£Œ")
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            confirmed_ipo_price = get_data("(í™•ì •)ê³µëª¨ê°€ê²©")
            subscription_rate = get_data("ì²­ì•½ê²½ìŸë¥ ")
            forecast_date = get_data("ìˆ˜ìš”ì˜ˆì¸¡ì¼")
            listing_date = get_data("ìƒì¥ì¼")
            print(f"âœ… {company} ê³µëª¨ì •ë³´ tab í¬ë¡¤ë§ ì™„ë£Œ")
        except Exception as e:
            print(f"âš  {company} ê³µëª¨ì •ë³´ íƒ­ ì ‘ê·¼ ì˜¤ë¥˜\n")

        # ì£¼ì£¼êµ¬ì„± íƒ­ìœ¼ë¡œ ì´ë™
        try:
            stockholder_info_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table/tbody/tr[5]/td/table/tbody/tr[1]/td[2]/a")
            stockholder_info_btn.click()
            # time.sleep(2) ëŒ€ì‹ : í˜ì´ì§€ ë‚´ì— "ê³µëª¨í›„" í…ìŠ¤íŠ¸ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            WebDriverWait(self.driver, 3).until(EC.presence_of_element_located(
                (By.XPATH, "//*[contains(text(), 'ê³µëª¨í›„')]")
            ))
            print(f"âœ… {company} ì£¼ì£¼êµ¬ì„± tab í´ë¦­ ì™„ë£Œ")
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
        except Exception as e:
            print(f"âš  {company} ì£¼ì£¼êµ¬ì„± íƒ­ ì ‘ê·¼ ì˜¤ë¥˜\n")

        # ê³µëª¨ í›„ ë°œí–‰ì£¼ì‹ìˆ˜ í¬ë¡¤ë§
        issued_shares = None
        try:
            public_after_td = soup.find("td", string="ê³µëª¨í›„")
            if public_after_td:
                parent_tr = public_after_td.find_parent("tr")
                next_tr = parent_tr.find_next_sibling("tr")
                if next_tr:
                    issued_shares_td = next_tr.find("td", string="ë°œí–‰ì£¼ì‹ìˆ˜")
                    if issued_shares_td:
                        issued_shares = issued_shares_td.find_next_sibling("td").text.strip()
        except Exception as e:
            print(f"âš  {company}ì˜ ê³µëª¨ í›„ ë°œí–‰ì£¼ì‹ìˆ˜ í¬ë¡¤ë§ ì˜¤ë¥˜\n")

        # ì£¼ì£¼êµ¬ì„± í…Œì´ë¸” í¬ë¡¤ë§
        composition_data={"ë³´í˜¸ì˜ˆìˆ˜ë§¤ë„ê¸ˆì§€":{}, "ìœ í†µê°€ëŠ¥":{}}
        try:
            composition_table=soup.find_all("table", {"class": "view_tb"})[2]
            if not composition_table:
                print(f"âš  {company} ì£¼ì£¼êµ¬ì„± í…Œì´ë¸” ì—†ìŒ")
                return
            current_section=None 
            result1={}
            result2={}
            category=None
            for row in composition_table.find_all("tr")[2:]:
                cells=row.find_all("td")
                one_row=[]
                if cells[0].get_text(strip=True) == "ë³´í˜¸ì˜ˆìˆ˜ë§¤ë„ê¸ˆì§€":
                    category=True
                    current_section=cells[1].get_text(strip=True)
                    for cell in cells[2:]:
                        one_row.append(cell.get_text(strip=True))
                    result1[current_section]=one_row
                elif cells[0].get_text(strip=True) == "ìœ í†µê°€ëŠ¥":
                    category=False
                    current_section=cells[1].get_text(strip=True)
                    for cell in cells[2:]:
                        one_row.append(cell.get_text(strip=True))
                    result2[current_section]=one_row
                elif cells[0].get_text(strip=True) in ["ë³´í˜¸ì˜ˆìˆ˜ ë¬¼ëŸ‰í•©ê³„","ìœ í†µê°€ëŠ¥ ì£¼ì‹í•©ê³„"]:
                    continue
                else:
                    current_section=cells[0].get_text(strip=True)
                    for cell in cells[1:]:
                        one_row.append(cell.get_text(strip=True))
                    if category:
                        result1[current_section]=one_row
                    if not category:
                        result2[current_section]=one_row
            composition_data["ë³´í˜¸ì˜ˆìˆ˜ë§¤ë„ê¸ˆì§€"]=result1
            composition_data["ìœ í†µê°€ëŠ¥"]=result2
        except:
            print(f"âš  {company} ì£¼ì£¼êµ¬ì„± table í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜")
        
        # ì¬ë¬´ì •ë³´ íƒ­ìœ¼ë¡œ ì´ë™
        try:
            financial_info_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table/tbody/tr[5]/td/table/tbody/tr[1]/td[3]/a")
            financial_info_btn.click()
            # time.sleep(2) ëŒ€ì‹ : ì¬ë¬´ì •ë³´ í…Œì´ë¸”ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            WebDriverWait(self.driver, 2).until(EC.presence_of_element_located(
                (By.XPATH, "//table[@class='view_tb']")
            ))
            print(f"âœ… {company} ì¬ë¬´ì •ë³´ tab í´ë¦­ ì™„ë£Œ")
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
        except Exception as e:
            print(f"âš  {company} ì¬ë¬´ì •ë³´ íƒ­ ì ‘ê·¼ ì˜¤ë¥˜: (ì•„ë§ˆ ìŠ¤íŒ©ì´ë‚˜ ë¦¬ì¸ )")
            self.spac_reits.append(company)

        # ì¬ë¬´ì •ë³´ í¬ë¡¤ë§

        financial_info = {}

        def parse_value(text):
            """í…ìŠ¤íŠ¸ì—ì„œ ì‰¼í‘œ, ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° í›„ ìˆ«ìë¡œ ë³€í™˜ (ì‹¤íŒ¨í•˜ë©´ None)"""
            text = text.replace(",", "")
            if text in ["", "-"]:
                return None
            try:
                return int(text)
            except ValueError:
                try:
                    return float(text)
                except ValueError:
                    return None
        
        def map_label(label):
            """ì¬ë¬´ì •ë³´ í…Œì´ë¸”ì˜ ë¼ë²¨ì„ JSONì˜ í‚¤ì— ë§ê²Œ ë§¤í•‘"""
            label = label.strip()
            label_clean = re.sub(r'^[\d\.\sâ… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]+', '', label)
            if "ìœ ë™ìì‚°" == label_clean:
                return "ìœ ë™ìì‚°"
            elif "ë¹„ìœ ë™ìì‚°" == label_clean:
                return "ë¹„ìœ ë™ìì‚°"
            # ìì‚°ì´ê³„ì™€ ìë³¸ì´ê³„ëŠ” êµ¬ë¶„í•´ì•¼ í•¨
            elif "ìì‚°ì´ê³„" == label_clean:
                return "ìì‚°ì´ê³„"
            elif "ìœ ë™ë¶€ì±„" == label_clean:
                return "ìœ ë™ë¶€ì±„"
            elif "ë¹„ìœ ë™ë¶€ì±„" == label_clean:
                return "ë¹„ìœ ë™ë¶€ì±„"
            elif "ë¶€ì±„ì´ê³„" == label_clean:
                return "ë¶€ì±„ì´ê³„"
            elif "ìë³¸ê¸ˆ" == label_clean:
                return "ìë³¸ê¸ˆ"
            elif "ìë³¸ì‰ì—¬ê¸ˆ" == label_clean:
                return "ìë³¸ì‰ì—¬ê¸ˆ"
            elif "ì´ìµì‰ì—¬ê¸ˆ" == label_clean:
                return "ì´ìµì‰ì—¬ê¸ˆ"
            elif "ê¸°íƒ€ìë³¸í•­ëª©" == label_clean:
                return "ê¸°íƒ€ìë³¸í•­ëª©"
            elif "ìë³¸ì´ê³„" == label_clean:
                return "ìë³¸ì´ê³„"
            elif "ë§¤ì¶œì•¡" == label_clean:
                return "ë§¤ì¶œì•¡"
            elif "ì˜ì—…ì´ìµ" == label_clean:
                return "ì˜ì—…ì´ìµ"
            elif "ë‹¹ê¸°ìˆœì´ìµ" == label_clean:
                return "ë‹¹ê¸°ìˆœì´ìµ"
            else:
                return None
        if soup:
            financial_table = soup.find("table", {"class": "view_tb"})
            if financial_table:
                rows = financial_table.find_all("tr")
                # ì²« 2í–‰ì€ í—¤ë”ë¡œ ê°€ì •í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.
                data_rows = rows[2:]
                for row in data_rows:
                    tds = row.find_all("td")
                    label = tds[0].get_text(strip=True)
                    key = map_label(label)
                    if key:
                        # ê° ì¬ë¬´ì •ë³´ í•­ëª©ì˜ 3ê°œ ê¸°ìˆ˜(ì˜ˆ: ì œ16ê¸°, ì œ15ê¸°, ì œ14ê¸°) ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                        values = [parse_value(td.get_text(strip=True)) for td in tds[1:4]]
                        financial_info[key] = values
                print(f"âœ… {company} ì¬ë¬´ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ\n")
            else:
                print(f"âš  {company} ì¬ë¬´ì •ë³´ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âš  {company} ì¬ë¬´ì •ë³´ í˜ì´ì§€ soup ìƒì„± ì‹¤íŒ¨.")

        # JSON ë°ì´í„° ì €ì¥
        company_data = {
            company: {
                "ìˆ˜ìš”ì˜ˆì¸¡": {
                    "(í¬ë§)ê³µëª¨ê°€ê²©": wanted_ipo_price,
                    "ë‹¨ìˆœ ê¸°ê´€ê²½ìŸë¥ ": competition_rate,
                    "ì˜ë¬´ë³´ìœ í™•ì•½ë¹„ìœ¨": lockup_ratio
                },
                "ê³µëª¨ì •ë³´": {
                    "(í™•ì •)ê³µëª¨ê°€ê²©": confirmed_ipo_price,
                    "ì²­ì•½ê²½ìŸë¥ ": subscription_rate,
                    "ìˆ˜ìš”ì˜ˆì¸¡ì¼": forecast_date,
                    "ìƒì¥ì¼": listing_date
                },
                "ì£¼ì£¼êµ¬ì„±": {
                    "ê³µëª¨í›„ ë°œí–‰ì£¼ì‹ìˆ˜": issued_shares,
                    "ì£¼ì£¼êµ¬ì„± table": composition_data
                },
                "ì¬ë¬´ì •ë³´": financial_info,
                "ì¢…ê°€ëŒ€ë¹„ë“±ë½ìœ¨": None
            }
        }

        self.result.append(company_data)
        self.driver.get(self.base_url)
        # time.sleep(2) ëŒ€ì‹ : ê¸°ë³¸ í˜ì´ì§€ì˜ ê²€ìƒ‰ì°½(FORM1)ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, "FORM1")))

    def scrape_data(self):
        """ ë‚ ì§œì— ë”°ë¼ í˜ì´ì§€ ì´ë™ í›„ í¬ë¡¤ë§ """
        prev_year = 2025
        for company in self.company_data.keys():
            date = datetime.strptime(self.company_data[company], "%Y-%m-%d")
            if date.year == 2025:
                self.crawl(company)
            else:
                if date.year != prev_year:
                    for _ in range(prev_year - date.year):
                        prev_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table[1]/tbody/tr[3]/td/table/tbody/tr[1]/td[1]/img[1]")
                        prev_btn.click()
                        # time.sleep(2) ëŒ€ì‹ : ê¸°ë³¸ í˜ì´ì§€ í…Œì´ë¸”ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                        WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.XPATH, "//*[@id='print']/table[1]")))
                    entire_btn = self.driver.find_element(By.XPATH, "//*[@id='print']/table[1]/tbody/tr[3]/td/table/tbody/tr[1]/td[14]/a")
                    entire_btn.click()
                    # time.sleep(2) ëŒ€ì‹ : ê²€ìƒ‰ì°½(FORM1)ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, "FORM1")))
                    self.base_url = self.driver.current_url
                    prev_year = date.year
                    self.crawl(company)
                else:
                    self.crawl(company)
        return self.result

    def save_to_database(self, data):
        if not data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # NEWBIE_PROJECT ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        base_dir = os.path.dirname(os.path.dirname(__file__))
        output_file = os.path.join(base_dir, "Finance_data", "IPOSTOCK_data.json")

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ìƒˆë¡œìš´ ë°ì´í„°ì™€ ë³‘í•©
        if os.path.exists(output_file):
            with open(output_file, mode='r', encoding='utf-8') as file:
                existing_data = json.load(file)
            print("ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        else:
            existing_data = []

        # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
        combined_data = existing_data + data

        # JSON íŒŒì¼ì— ë³‘í•©ëœ ë°ì´í„° ì €ì¥
        with open(output_file, mode='w', encoding='utf-8') as file:
            json.dump(combined_data, file, ensure_ascii=False, indent=4)

        print(f"ë°ì´í„°ê°€ {output_file}ì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def run(self):
        try:
            for json in ["KIND_data_2523.json","KIND_data_2220.json","KIND_data_1917.json","KIND_data_1614.json"]:
                self.base_url="http://www.ipostock.co.kr/sub03/ipo08.asp?str4=2025&str5=all"
                self.start_browser("./Finance_data/"+json)

                # # ë‚ ì§œ ë²”ìœ„ ì„¤ì • ë° ì‚¬ì´íŠ¸ ì ‘ì†
                # self.select_date_range(start_date, end_date)

                # ë°ì´í„° í¬ë¡¤ë§
                data = self.scrape_data()

                # ë°ì´í„° ì €ì¥
                self.save_to_database(data)
                self.driver.quit()
        finally:
            print("í¬ë¡¤ë§ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            print(f"âš  ê²€ìƒ‰ ì‹¤íŒ¨ ëª©ë¡(ì´ë¦„ì´ ë‹¤ë¥´ê±°ë‚˜ ì—†ëŠ”ê²ƒ):{self.search_fail_list}\n")
            print(f"âš  ì¬ë¬´ì •ë³´ ì—†ëŠ”ê²ƒ(ìŠ¤íŒ© or ë¦¬ì¸ ): {self.spac_reits}")

if __name__ == "__main__":
    output_directory = os.path.join(os.path.dirname(__file__), "output")

    os.makedirs(output_directory, exist_ok=True)

    crawler = IpostockCrawler(output_dir=output_directory)
    crawler.run()

