from crawling.base_crawler import BaseCrawler
from crawling.utils.logger import setup_logger

from selenium.webdriver.chrome.webdriver import WebDriver

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys

from typing import Dict, List
import pandas as pd
import time
import os
import sys
import json

import random

from image_ocr import process_image_ocr

#-----------------------------------------
config = ('-l  kor+kor_vert+eng')

proxy_list = [
    "15.236.203.245:3128",
    "98.80.66.1:10018",
    "44.219.175.186:80"
]
#-----------------------------------------

class DcCrawler1(BaseCrawler):
    def __init__(self, output_dir: str, start_page: int = 1, end_page: int = 500, proxy: str = None):
        self.output_dir = output_dir
        self.start_page = start_page
        self.end_page = end_page  # 500í˜ì´ì§€ê¹Œì§€ ìë™ í¬ë¡¤ë§
        # ê°œë…ê¸€, ë¦¬ìŠ¤íŠ¸ 100ê°œì”©ìœ¼ë¡œ ì§„í–‰
        self.base_url_template = "https://gall.dcinside.com/mgallery/board/lists/?id=kospi&list_num=100&sort_type=N&exception_mode=recommend&search_head=&page={}"
        self.driver = None
        self.reviews: List[Dict[str, str]] = []
        self.logger = setup_logger(log_file='./crawling/utils/dc.log')  # Logger ì„¤ì •
        self.proxy = None #random.choice(proxy_list) 

    
    def is_allowed_url(self, url: str) -> bool:
        """
        âœ… robots.txtì—ì„œ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ URLì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
        """
        disallowed_paths = [
            "/gallog/", "/api/", "/board/47", "/board/stock_new/", "/board/cat/",
            "/board/dog/", "/board/d_fighter_new1", "/board/government/", "/board/metakr/",
            "/board/salgoonews/", "/board/intl_"
        ]

        # âŒ Disallowëœ ê²½ë¡œ í¬í•¨ ì—¬ë¶€ í™•ì¸
        for path in disallowed_paths:
            if path in url:
                return False
        return True  # âœ… í¬ë¡¤ë§ ê°€ëŠ¥ URL

    def start_browser(self):
        """
        í¬ë¡¬ ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
        """
        self.logger.info("ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤â€¦")
        chrome_options = Options()
        chrome_options.add_argument('â€”disable-blink-features=AutomationControlled')
        # chrome_options.add_argument('â€”start-maximized')  # ë¸Œë¼ìš°ì € ìµœëŒ€í™”ë¡œ ì‹œì‘
        chrome_options.add_argument("--headless")  # ğŸ”¹ ê¸°ë³¸ì ìœ¼ë¡œ í¬ë¡¬ ì°½ì„ ìˆ¨ê¹€
        chrome_options.add_argument("--disable-gpu")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ GPU ë Œë”ë§ ë°©ì§€
        chrome_options.add_argument("--window-size=1400,600")  # í¬ê¸° ì„¤ì •
        # chrome_options.add_argument("--incognito") #ì‹œí¬ë¦¿ ëª¨ë“œ : ì¿ í‚¤ ì—†ìŒ

        # ë¸Œë¼ìš°ì € User-Agent ì„¤ì • (ì‹¤ì œ ë¸Œë¼ìš°ì € User-Agentë¡œ ë³€ê²½)
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"user-agent={user_agent}")

        #í”„ë¡ì‹œ ì‚¬ìš©
        if self.proxy:
            chrome_options.add_argument(f"--proxy-server=http://{self.proxy}")

        service = Service(ChromeDriverManager().install())  # ChromeDriver ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ì‹¤í–‰
        self.browser = webdriver.Chrome(service=service, options=chrome_options)

        # âœ… navigator.webdriver ì†ì„± ìš°íšŒ (ìë™í™” íƒì§€ ì°¨ë‹¨)
        self.browser.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        # âœ… CDP(Chrome DevTools Protocol)ì—ì„œ User-Agent ë³€ê²½ ì ìš©
        self.browser.execute_cdp_cmd("Network.setUserAgentOverride", {"userAgent": user_agent})

        self.logger.info("ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„±ê³µ!")

    def scrape_reviews(self):
        self.logger.info("í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤â€¦")
        self.start_browser()

        for page_num in range(self.start_page, self.end_page + 1):
            self.reviews.clear()  # ë§¤ í˜ì´ì§€ë§ˆë‹¤ reviews ì´ˆê¸°í™”
            page_url = self.base_url_template.format(page_num)
            self.logger.info(f"ğŸ“Œ {page_num}í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {page_url}")



            # Step 1: í˜ì´ì§€ ì ‘ì†
            # ğŸ”¹ í˜ì´ì§€ê°€ ë³€ê²½ë  ë•Œë§Œ ë¸Œë¼ìš°ì €ë¥¼ ë³´ì´ê²Œ ì„¤ì •
            self.browser.set_window_position(0, 0)  # í™”ë©´ì— ë³´ì´ë„ë¡ ë³€ê²½
            self.browser.get(page_url)
            time.sleep(1)  # í˜ì´ì§€ê°€ ë³´ì¼ ì‹œê°„ì„ ì•½ê°„ ì¤Œ
            self.browser.minimize_window()  # ğŸ”¹ ì°½ì„ ìµœì†Œí™”í•˜ì—¬ ìˆ¨ê¹€ 
            self.logger.info(f"URL ì—´ê¸° ì„±ê³µ: {page_url}")
            time.sleep(random.uniform(2, 5))  # ëœë¤ ë”œë ˆì´ ì ìš© (ì„œë²„ ë¶€í•˜ ë°©ì§€)


        # Step 2: ëª©ë¡ì—ì„œ ìš”ì†Œ ë°›ì•„ì˜¤ê¸° & ê¸€ ê±°ë¥´ê¸°(ìš°ì„  ë‚˜ì¤‘ì— ì²˜ë¦¬)
            try:
                BOX_PATH = "/html/body/div[2]/div[3]/main/section[1]/article[2]/div[2]/table/tbody/tr"
                WebDriverWait(self.browser, 10).until(EC.visibility_of_element_located((By.XPATH, BOX_PATH)))
                #ê¸€ ìˆœì„œ íŒŒì•…ìš© ì§€ë‚˜ì³ì•¼ í•  ê¸€ ê°œìˆ˜
                non_review_elements = self.browser.find_elements(By.XPATH, '//tr[@class="ub-content "]')
                pass_num = len(non_review_elements)
                self.logger.info(f"ì´ {pass_num}ê°œì˜ ê¸€ì´ ìš´ì˜ì ì‘ì„±ê¸€ì…ë‹ˆë‹¤.")

                review_elements = self.browser.find_elements(By.XPATH, '//tr[@class="ub-content us-post"]')
                self.logger.info(f"ì´ {len(review_elements)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                for i, review in enumerate(review_elements):
                    time.sleep(random.uniform(5, 10))  # âœ… Crawl-delay ì¤€ìˆ˜ (ëœë¤ ë”œë ˆì´ ì ìš©)
                    try:
                        category = review.find_element(By.XPATH, f"{BOX_PATH}[{pass_num+i+1}]/td[2]").text
                        #ì ìš© ì•ˆë¨ ìˆ˜ì • í•„ìš”
                        # if category.strip() in ["ë¹¨í†µâ¤", "ê°œì†Œë¦¬", "ğŸë‹¤ì´"]:
                        #     pass
                        title = review.find_element(By.XPATH, f"{BOX_PATH}[{pass_num+i+1}]/td[3]/a[1]").text
                        date = review.find_element(By.XPATH, f"{BOX_PATH}[{pass_num+i+1}]/td[5]").get_attribute("title")
                        views = review.find_element(By.XPATH, f"{BOX_PATH}[{pass_num+i+1}]/td[6]").text
                        likes = review.find_element(By.XPATH, f"{BOX_PATH}[{pass_num+i+1}]/td[7]").text

                    except Exception as e:
                        self.logger.error(f"ë¦¬ë·° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

        # Step 3: ë³¸ë¬¸ ì•ˆìœ¼ë¡œ ì…ì¥
                    try:
                        page_url = review.find_element(By.XPATH, f'{BOX_PATH}[{pass_num + i + 1}]/td[3]/a[1]').get_attribute("href")

                        # âœ… `robots.txt`ì—ì„œ í—ˆìš©ëœ URLë§Œ í¬ë¡¤ë§
                        if not self.is_allowed_url(page_url):
                            self.logger.warning(f"ì°¨ë‹¨ëœ ê²Œì‹œíŒ URL: {page_url} â†’ í¬ë¡¤ë§ ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue

                        self.browser.execute_script("window.open('');")  # ìƒˆ íƒ­ ì—´ê¸°
                        self.browser.switch_to.window(self.browser.window_handles[-1])
                        self.browser.get(page_url)  # í•´ë‹¹ URLë¡œ ì´ë™
                        # time.sleep(1)
                        self.logger.info(f"{page_num}ë²ˆì§¸ í˜ì´ì§€, {i+1}ë²ˆì§¸ ê¸€ë¡œ ì…ì¥í•©ë‹ˆë‹¤.")


        # Step 4: ë³¸ë¬¸ ì•ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        try:
                            WebDriverWait(self.browser, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class="writing_view_box"]')))
                            unlikes = self.browser.find_element(By.XPATH, '//p[@class="down_num"]').text
                            # zzbang_divì€ ë‹¨ìˆœ ê·¸ëƒ¥ ì‚¬ì§„ë“¤ì´ë¼ í¬í•¨ ì•ˆì‹œí‚´
                            contents = self.browser.find_elements(By.XPATH, '//div[@class="writing_view_box"]/div[@class="write_div"]')
                            self.logger.info(f"ë³¸ë¬¸ ì•ˆì—ì„œ write_div ê°œìˆ˜: {len(contents)}ê°œ")

                            content_list = []
                            img_num = 0  # âœ… imgwrapë¡œ ì‹œì‘í•˜ëŠ” div ê°œìˆ˜ ì¹´ìš´íŠ¸ ë³€ìˆ˜
                            img_url_list = []
                            text_from_img_list = []
                            

                            for j, content in enumerate(contents):
                                # âœ… write_div ë‚´ë¶€ì— divê°€ ìˆëŠ”ì§€ í™•ì¸
                                inner_divs = content.find_elements(By.XPATH, './div')
                                self.logger.info(f"ë³¸ë¬¸ ì•ˆì—ì„œ write_divì•ˆì˜ div ê°œìˆ˜: {len(inner_divs)}ê°œ")

                                if inner_divs:  # âœ… í•˜ìœ„ divê°€ ìˆì„ ê²½ìš°
                                    self.logger.info("ë³¸ë¬¸ì˜ êµ¬ì¡°ê°€ divë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤.")
                                    for inner_div in inner_divs:
                                        try:
                                            # âœ… inner_div ë‚´ë¶€ì˜ ëª¨ë“  div ì°¾ê¸°
                                            nested_divs = inner_div.find_elements(By.XPATH, './div')
                                            # self.logger.info("ì§€ê¸ˆ divì•ˆì˜ divì˜ class ì´ë¦„ì„ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.")

                                            for nested_div in nested_divs:
                                                class_name = nested_div.get_attribute("class")  # âœ… divì˜ class ì†ì„± ê°€ì ¸ì˜¤ê¸°

                                                # âœ… "imgwrap no"ë¡œ ì‹œì‘í•˜ëŠ” divê°€ ìˆëŠ” ê²½ìš° ì¹´ìš´íŠ¸ ì¦ê°€
                                                if class_name.startswith("imgwrap"):
                                                    img_num += 1

                                                    img_elements = nested_div.find_elements(By.XPATH, './/img')
                                                    if img_elements:
                                                        # âœ… í•´ë‹¹ div ë‚´ë¶€ì˜ img íƒœê·¸ ì°¾ê¸°
                                                        img_src = img_elements[0].get_attribute("src")

                                                        # âœ… OCR ì‹¤í–‰
                                                        text_from_img = ""
                                                        text_from_img = process_image_ocr(img_src)

                                                        img_url_list.append(img_src)
                                                        text_from_img_list.append(text_from_img)

                                                # âœ… "ai_wrap" í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divëŠ” ë¬´ì‹œ
                                                if "ai_wrap" in class_name:
                                                    continue

                                        except Exception as e:
                                            self.logger.error(f"div class ì´ë¦„ í™•ì¸ ì¤‘ error ë°œìƒ: {e}")

                                        # âœ… í•˜ìœ„ divì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                        text_content = inner_div.get_attribute("innerText").strip()
                                        
                                        if text_content:
                                            content_list.append(text_content)

                                else:  # âœ… í•˜ìœ„ divê°€ ì—†ì„ ê²½ìš°, write_div ìì²´ì˜ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                                    self.logger.info("ë³¸ë¬¸ì˜ êµ¬ì¡°ê°€ div ì—†ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                                    text_content = content.get_attribute("innerText").strip()
                                    if text_content:
                                        content_list.append(text_content)

                            self.logger.info(f"ë³¸ë¬¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤: {len(content_list)}ê°œì˜ ë¬¸ë‹¨ì„ ì €ì¥í•¨.")
                            self.logger.info(f"ì´ë¯¸ì§€ div(imgwrap) ê°œìˆ˜: {img_num}")
                            self.logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œí•œ ì´ë¯¸ì§€ ìˆ˜: {len(img_url_list)}")

        # # Step 5: ë³¸ë¬¸ ì•ˆ ì´ë¯¸ì§€ ì¶”ì¶œ -> ë³¸ë¬¸ ì¶”ì¶œì—ì„œ ì§„í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ìˆ˜ì •

        # Step 6: ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
                            try:
                                comments = self.browser.find_elements(By.XPATH, '//div[@class="comment_box"]//p[@class="usertxt ub-word"]')
                                comment_list = []
                                for l, comment in enumerate(comments):
                                    comment_list.append(comment.text)  # âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
                
                                self.logger.info(f"ì¶”ì¶œëœ ëŒ“ê¸€ ê°œìˆ˜: {len(comment_list)}")
                            except Exception as e:
                                self.logger.error(f"ëŒ“ê¸€ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

                            self.reviews.append({
                                "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€": page_num,
                                "ë§ë¨¸ë¦¬": category,
                                "title": title,
                                "ì‘ì„±ì‹œê°„" : date,
                                "ì¡°íšŒìˆ˜": views,
                                "ì¶”ì²œìˆ˜": likes,
                                "ë¹„ì¶”ì²œìˆ˜": unlikes,
                                "ë§í¬" : page_url,
                                "ë³¸ë¬¸" : json.dumps(content_list, ensure_ascii=False),
                                "ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ìˆ˜": img_num,
                                "ì´ë¯¸ì§€ ë§í¬": json.dumps(img_url_list, ensure_ascii=False),
                                "ì¶”ì¶œ í…ìŠ¤íŠ¸ ë§í¬" : json.dumps(text_from_img_list, ensure_ascii=False),
                                "ëŒ“ê¸€" : comment_list
                            })

                        except Exception as e:
                            self.logger.error(f"ë³¸ë¬¸ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

        
        # Step 7: í˜ì´ì§€ ë‹«ê³  ë³µê·€(ë‹¤ìŒ í˜ì´ì§€ ì¤€ë¹„)
                        self.browser.close()
                        self.logger.info("ê¸€ì„ ë‹«ê³  ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
                        self.logger.info("--------------------------------------------------------")
                        # time.sleep(1)
                        self.browser.switch_to.window(self.browser.window_handles[0])


                    except Exception as e:
                        self.logger.error(f"ë³¸ë¬¸ì— ì…ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.: {e}")

                    # âœ… 10ê°œ ì´ìƒì´ë©´ ì €ì¥ ì‹¤í–‰ (for-loop ë‚´ë¶€ì—ì„œ ì‹¤í–‰)
                    if len(self.reviews) >= 10:
                        self.logger.info("ğŸ“Œ 10ê°œ ë‹¨ìœ„ë¡œ ì €ì¥ ì‹¤í–‰")
                        self.save_to_database(page_num)

                # âœ… for-loop ì¢…ë£Œ í›„, ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì €ì¥
                if self.reviews:  
                    self.logger.info("ğŸ“Œ ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì €ì¥ ì‹¤í–‰")
                    self.save_to_database(page_num)

            except Exception as e:
                self.logger.error(f"ë¦¬ë·° ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

        # Step 7: ë¸Œë¼ìš°ì € ì¢…ë£Œ
        self.logger.info("í¬ë¡¤ë§ ì™„ë£Œ. ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        self.browser.quit()

    def save_to_database(self, page_num):
        """
        âœ… 10ê°œ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§í•œ ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        """
        if not self.reviews:
            self.logger.info("ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        output_path = os.path.join(self.output_dir, f"dc_pages_{page_num}page.csv")

        # âœ… DataFrame ë³€í™˜ (ì»¬ëŸ¼ ìˆœì„œ ì§€ì •)
        df = pd.DataFrame(self.reviews, columns=[
            "í˜„ì¬ í¬ë¡¤ë§ í˜ì´ì§€", "ë§ë¨¸ë¦¬", "title", "ì‘ì„±ì‹œê°„", "ì¡°íšŒìˆ˜", "ì¶”ì²œìˆ˜", "ë¹„ì¶”ì²œìˆ˜",
            "ë§í¬", "ë³¸ë¬¸", "ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ìˆ˜", "ì´ë¯¸ì§€ ë§í¬", "ì¶”ì¶œ í…ìŠ¤íŠ¸ ë§í¬", "ëŒ“ê¸€"
        ])

        # âœ… CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì €ì¥ ë°©ì‹ ê²°ì •
        file_exists = os.path.exists(output_path)

        if file_exists:
            # âœ… ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ append (header ì—†ì´ ì¶”ê°€)
            df.to_csv(output_path, index=False, encoding="utf-8-sig", mode="a", header=False)
            self.logger.info(f"ğŸ“Œ ê¸°ì¡´ CSV íŒŒì¼ì— 10ê°œ ì¶”ê°€ ì €ì¥: {output_path}")
        else:
            # âœ… ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ íŒŒì¼ ìƒì„± (header í¬í•¨)
            df.to_csv(output_path, index=False, encoding="utf-8-sig", mode="w", header=True)
            self.logger.info(f"ğŸ“Œ ìƒˆ CSV íŒŒì¼ ìƒì„± ë° ì €ì¥: {output_path}")

        # âœ… ì €ì¥ í›„ self.reviews ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.reviews.clear()