import numpy as np
import pandas as pd
from collections import Counter
from tqdm import tqdm
from konlpy.tag import Mecab, Okt

# ê° ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ì§‘ ë¡œë“œ í•¨ìˆ˜
def load_category_vocab(file_paths):
    category_vocab = {}
    for category, file_path in file_paths.items():
        with open(file_path, 'r', encoding='utf-8') as f:
            category_vocab[category] = [line.strip() for line in f.readlines()]
    return category_vocab

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì„ íƒ (Mecabì´ ê°€ì¥ ë¹ ë¦„, ì—†ìœ¼ë©´ Okt ì‚¬ìš©)
def get_tokenizer(tokenizer_name="mecab"):
    if tokenizer_name == "mecab":
        return Mecab(dicpath="/opt/homebrew/lib/mecab/dic/mecab-ko-dic")  # ì‚¬ì „ ê²½ë¡œ ì§€ì •
    elif tokenizer_name == "okt":
        return Okt()
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì…ë‹ˆë‹¤. 'mecab' ë˜ëŠ” 'okt'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜
def tokenize_text(text, tokenizer, mode="noun"):
    """
    í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì›í•˜ëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    :param text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    :param tokenizer: í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´
    :param mode: ì¶”ì¶œí•  í’ˆì‚¬ ('noun' = ëª…ì‚¬, 'morph' = ëª¨ë“  í˜•íƒœì†Œ)
    :return: í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸
    """
    if mode == "noun":
        return tokenizer.nouns(text)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ
    elif mode == "morph":
        return tokenizer.morphs(text)  # ëª¨ë“  í˜•íƒœì†Œ ì¶”ì¶œ
    else:
        raise ValueError("modeëŠ” 'noun' ë˜ëŠ” 'morph' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

# IDF ê³„ì‚° í•¨ìˆ˜
def compute_idf(docs, vocab):
    N = len(docs)
    idf = {}
    for word in vocab:
        df = sum(1 for doc in docs if word in doc)
        idf[word] = np.log((N + 1) / (df + 1)) + 1
    return idf

# Semi TF-IDF ê³„ì‚° í•¨ìˆ˜
def semi_tfidf(docs, category_vocab, tokenizer, mode="noun"):
    """
    í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ Semi TF-IDF ê³„ì‚° í•¨ìˆ˜
    
    :param docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    :param category_vocab: ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ì§‘ (dict)
    :param tokenizer: í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´
    :param mode: í˜•íƒœì†Œ ë¶„ì„ ëª¨ë“œ ('noun' ë˜ëŠ” 'morph')
    :return: ì¹´í…Œê³ ë¦¬ë³„ TF-IDF ì ìˆ˜
    """
    # ì „ì²´ ë‹¨ì–´ ì§‘í•© ìƒì„±
    total_vocab = set(word for vocab in category_vocab.values() for word in vocab)
    
    # IDF ê³„ì‚°
    idf = compute_idf(docs, total_vocab)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì €ì¥
    category_scores = {f'tfidf_{category}': [] for category in category_vocab.keys()}
    
    for doc in tqdm(docs, desc="ğŸ“„ TF-IDF ì§„í–‰"):
        words = tokenize_text(doc, tokenizer, mode)  # í˜•íƒœì†Œ ë¶„ì„ ì ìš©
        word_count = Counter(words)  # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        
        for category, vocab in category_vocab.items():
            score = sum(word_count[word] * idf.get(word, 0) for word in vocab)
            category_scores[f'tfidf_{category}'].append(score)
    
    return category_scores

# ì‹¤í–‰ í•¨ìˆ˜
def run_tfidf(news_file, category_files, tokenizer_name="mecab", mode="noun"):
    """
    í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ Semi TF-IDF ì‹¤í–‰ í•¨ìˆ˜
    
    :param news_file: ë‰´ìŠ¤ ë°ì´í„° (DataFrame)
    :param category_files: ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ì§‘ íŒŒì¼ ê²½ë¡œ (dict)
    :param tokenizer_name: ì‚¬ìš©í•  í˜•íƒœì†Œ ë¶„ì„ê¸° ('mecab' ë˜ëŠ” 'okt')
    :param mode: í˜•íƒœì†Œ ë¶„ì„ ëª¨ë“œ ('noun' ë˜ëŠ” 'morph')
    :return: TF-IDF ì ìˆ˜ê°€ í¬í•¨ëœ DataFrame
    """
    df = news_file
    category_vocab = load_category_vocab(category_files)
    tokenizer = get_tokenizer(tokenizer_name)
    
    # ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    docs = df['Body_processed'].astype(str).tolist()
    
    # Semi TF-IDF ê³„ì‚°
    category_scores = semi_tfidf(docs, category_vocab, tokenizer, mode)
    
    # ê¸°ì¡´ DataFrameì— ì ìˆ˜ ì¶”ê°€
    for category, scores in category_scores.items():
        df[category] = scores
    
    # ëª¨ë“  TF-IDF ì ìˆ˜ê°€ 0ì¸ ê²½ìš° ë°ì´í„° ì‚­ì œ
    tfidf_columns = [col for col in df.columns if col.startswith('tfidf_')]
    df = df[df[tfidf_columns].sum(axis=1) != 0]
    
    return df