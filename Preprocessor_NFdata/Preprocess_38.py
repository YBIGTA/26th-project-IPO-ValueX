import json
import re
from datetime import datetime, timedelta
from collections import defaultdict

# âœ… Step 1: ë‚ ì§œ í•„í„°ë§ (datecutting.py ê¸°ëŠ¥)
def filter_by_date(input_data, kind_data):
    kind_dict = {entry["ê¸°ì—…ëª…"]: datetime.strptime(entry["ìƒì¥ì¼"], "%Y-%m-%d") for entry in kind_data}
    valid_companies = set(kind_dict.keys())
    
    def parse_date(date_str):
        date_str = date_str.strip()
        
        if not date_str:
            return None  # ë¹ˆ ë¬¸ìì—´ì´ë©´ None ë°˜í™˜
        
        for fmt in ("%Y/%m/%d", "%Y-%m-%d", "%m/%d/%Y"):
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        print(f"ğŸš¨ ê²½ê³ : '{date_str}'ëŠ” ì¸ì‹í•  ìˆ˜ ì—†ëŠ” ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤. ë°ì´í„° ë¬´ì‹œ.")
        return None  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    
    filtered_data = []
    for entry in input_data:
        company = entry.get("ê¸°ì—…ëª…", "").strip()
        post_date_str = entry.get("ë‚ ì§œ", "").strip()

        if not post_date_str:
            print(f"ğŸš¨ ê²½ê³ : '{company}'ì˜ ë‚ ì§œ í•„ë“œê°€ ì—†ìŒ. ë°ì´í„° ë¬´ì‹œ.")
            continue

        post_date = parse_date(post_date_str)
        if post_date is None:
            continue

        if company in valid_companies:
            ipo_date = kind_dict[company]
            start_date = ipo_date - timedelta(days=30)

            if start_date <= post_date <= ipo_date:
                filtered_data.append(entry)
    
    return filtered_data

# âœ… Step 2: ì¢…ëª©í† ë¡ ë°© ìš©ì–´ ë³€í™˜ (transform_word.py ê¸°ëŠ¥)
def transform_text(data):
    stock_terms = {
        # ì£¼ê°€ ìƒìŠ¹ ê´€ë ¨
    "ë”°ìƒ": "ìƒí•œê°€ 2ë²ˆ", "ë”°ë¸”": "2ë°° ìƒìŠ¹", "ê°•ìƒë”°": "ê°•í•œ ìƒí•œê°€ í›„ ë§¤ìˆ˜",
    "ì©œìƒ": "ìƒí•œê°€ì—ì„œ ì†Œí­ ì›€ì§ì„", "ìˆ˜ì§ìƒìŠ¹": "ê¸‰ë“±", "ì—°ìƒ": "ì—°ì† ìƒí•œê°€",
    "ì¥ëŒ€ì–‘ë´‰": "ì£¼ê°€ì˜ í° í­ ìƒìŠ¹", "ì‹œì´ˆê°€": "ì¥ ì‹œì‘ ê°€ê²©", "ë–¡ìƒ" : "ì£¼ê°€ê°€ ë§ì´ ì˜¤ë¦„",

    # ì£¼ê°€ í•˜ë½ ê´€ë ¨
    "ë”°í•˜": "í•˜í•œê°€ 2ë²ˆ", "í•˜ë”°": "í•˜í•œê°€ì—ì„œ ë§¤ìˆ˜", "ì¥ëŒ€ ìŒë´‰": "ê¸´ í•˜ë½ ìº”ë“¤",
    "ë¬¼ëŸ‰ ì¶œíšŒ": "ëŒ€ëŸ‰ ë§¤ë„ ë°œìƒ", "ì†ì ˆ": "ì†ì‹¤ í™•ì • ë§¤ë„", "ìµì ˆ": "ì´ìµ ì‹¤í˜„ ë§¤ë„",
    "ë¬¼ë¦¼": "ê³ ì  ë§¤ìˆ˜ í›„ í•˜ë½", "ë¬¼íƒ€ê¸°": "ì¶”ê°€ ë§¤ìˆ˜", "ë¶ˆíƒ€ê¸°": "ìƒìŠ¹ì¥ì—ì„œ ì¶”ê°€ ë§¤ìˆ˜",
    "í•œê°•": "ì „ì¬ì‚° ì†ì‹¤", "ì´ˆìƒ": "ì´ˆìƒì§‘ ë¶„ìœ„ê¸°", "ë–¡ë½" : "ì£¼ê°€ê°€ ë§ì´ ë–¨ì–´ì§", "ë¬¼ë ¸ë‹¤" : "ì†í•´ ë§ì´ ë´¤ë‹¤", "ë¬¼ë¦¼" : "ì†í•´ ë§ì´ ë´„",

    # íˆ¬ì ì‹¬ë¦¬ ê´€ë ¨
    "ì¡´ë²„": "ì¥ê¸° ë³´ìœ ", "ë‡Œë™ë§¤ë§¤": "ê°ì •ì  ë§¤ë§¤", "ëª°ë¹µ": "ì „ ì¬ì‚° íˆ¬ì",
    "ê°œë¯¸": "ê°œì¸ íˆ¬ìì", "ê¸°ê´€": "ê¸°ê´€ íˆ¬ìì", "ì™¸ì¸": "ì™¸êµ­ì¸ íˆ¬ìì",
    "ì–‘ì „": "ì£¼ê°€ê°€ ì „ì¼ ëŒ€ë¹„ ìƒìŠ¹", "ìŒì „": "ì£¼ê°€ê°€ ì „ì¼ ëŒ€ë¹„ í•˜ë½",
    "ê°œì¡ì£¼": "ë³€ë™ì„±ì´ í° ì¢…ëª©", "ì¡ì£¼": "ì €í‰ê°€ëœ ì¢…ëª©", "ëŒ€ì¥ì£¼": "ì—…ì¢… ëŒ€í‘œì£¼",
    "í…Œë§ˆì£¼": "íŠ¹ì • ì´ìŠˆë¡œ ì›€ì§ì´ëŠ” ì£¼ì‹", "ì‘ì „ì£¼": "ì„¸ë ¥ë“¤ì´ ì¸ìœ„ì ìœ¼ë¡œ ì˜¬ë¦¬ëŠ” ì£¼ì‹",
    "VI ë°œë™": "ë³€ë™ì„± ì™„í™” ì¥ì¹˜ ë°œë™", "ê³µë§¤ë„": "ì£¼ì‹ì„ ë¹Œë ¤ì„œ ë§¤ë„ í›„, í•˜ë½ ì‹œ ì°¨ìµ ì‹¤í˜„",
    "ì•ˆí‹°": "íŠ¹ì • ì¢…ëª©ì— ëŒ€í•´ ë¶€ì •ì ì¸ ì‚¬ëŒë“¤", "ë˜ì¡Œë‹¤" :" ë§í•´ì„œ í¬ê¸°í–ˆë‹¤", "í™€ë”©" : "ì‚¬ê³ íŒ”ì§€ ë§ê³  ê¸°ë‹¤ë¦¼",

    # ê°ì • í‘œí˜„ (ë¶€ì •ì  ë¶„ìœ„ê¸° í‘œí˜„)
    "â€¦": "ì‹¤ë§", "ã…œã…œã…œ": "ìŠ¬í””", "ã… ã… ã… ": "ì¢Œì ˆ", "í•˜â€¦": "í•œìˆ¨",
    "í—ˆí—ˆâ€¦": "ì–´ì´ì—†ìŒ", "í›„â€¦": "ì‹¤ë§", "ì°¸ë‚˜â€¦": "í™©ë‹¹", "ì•„ë‹ˆâ€¦": "ë‹¹í™©",
    "ì–´íœ´â€¦": "ë‹µë‹µí•¨", "ì´ê²Œ ë­ì•¼â€¦": "ê¸°ëŒ€ê°€ ë¬´ë„ˆì§", "ë§í–ˆë‹¤â€¦": "ì ˆë§",
    "ëë‚¬ë‹¤â€¦": "í¬ë§ ì—†ìŒ", "ë‹µ ì—†ë‹¤â€¦": "í•´ê²°ì±… ì—†ìŒ",
    "í˜ë¹ ì§€ë„¤â€¦": "ì§€ì¹¨", "ë©˜ë¶•": "ì •ì‹ ì  ì¶©ê²©", "ê°œê°™ì´" : "ì—„ì²­ë‚˜ê²Œ"
    }
    
    def clean_text(text):
        if text is None:
            return ""
        text = text.replace("\n", " ")  # ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r"[\~\-\=\+\*\[\]\(\)]", "", text)  # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (~, -, =, +, *, [], (), ë“±)
        text = re.sub(r"ã…‹{2,}", "ã…‹ã…‹", text)  # ã…‹ã…‹ã…‹ ê°™ì€ ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
        text = re.sub(r"ã…{2,}", "ã…ã…", text)  # ã…ã…ã… ê°™ì€ ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
        text = re.sub(r"\?{2,}", "?", text)  # ??? ê°™ì€ ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
        text = re.sub(r"\!{2,}", "!", text)  # !!! ê°™ì€ ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
        text = re.sub(r"[\s]+", " ", text).strip()  # ì—°ì†ëœ ê³µë°± ì œê±°
        return text 
    
    def preprocess_text(text):
        for term, replacement in stock_terms.items():
            text = text.replace(term, replacement)
        return clean_text(text)
    
    for entry in data:
        entry["ë‚´ìš©"] = preprocess_text(entry["ë‚´ìš©"])
    return data

# âœ… Step 3: ë°ì´í„° ê·¸ë£¹í™” ë° ì •ë¦¬ (form_to_train.py ê¸°ëŠ¥, ê°œì„  ë²„ì „)
def group_and_clean_data(data):
    grouped_data = defaultdict(list)
    company_views = defaultdict(list)
    
    def safe_int(value):
        try:
            return int(value) if value and str(value).isdigit() else None
        except ValueError:
            return None
    
    for entry in data:
        if (views := safe_int(entry.get("ì¡°íšŒ"))) and views > 0:
            company_views[entry["ê¸°ì—…ëª…"]].append(views)
    
    global_avg_views = sum(v for views in company_views.values() for v in views) // len(company_views) if company_views else 0
    company_avg_views = {company: (sum(views) // len(views) if views else global_avg_views) for company, views in company_views.items()}
    
    def is_valid_text(text):
        text = text.strip()
        if re.fullmatch(r"\.*", text):  
            return False  
        if len(text.split()) == 1:  
            return False  
        if len(text.replace(" ", "")) <= 5:  
            return False  
        words = text.split()
        midpoint = len(words) // 2
        if len(words) > 4 and words[:midpoint] == words[midpoint:]:  
            return False
        return True  
    
    for entry in data:
        company = entry["ê¸°ì—…ëª…"]
        text = f"{entry.get('ì œëª©', '')} {entry.get('ë‚´ìš©', '')}".strip()
        
        # âœ… ì¡°íšŒìˆ˜ ì²˜ë¦¬ (ë¹ˆ ê°’ì´ê±°ë‚˜ 0ì´ë©´ í‰ê· ê°’ ì‚¬ìš©)
        views = safe_int(entry.get("ì¡°íšŒ"))  # ì¡°íšŒìˆ˜ ë³€í™˜
        if views is None or views == 0:  # ì¡°íšŒìˆ˜ê°€ ì—†ê±°ë‚˜ 0ì´ë©´ í‰ê·  ì¡°íšŒìˆ˜ ì‚¬ìš©
            views = company_avg_views.get(company, global_avg_views)

        # âœ… ì¶”ì²œìˆ˜ ì²˜ë¦¬ (ë¹ˆ ê°’ì´ë©´ 0)
        recommendations = safe_int(entry.get("ì¶”ì²œ")) or 0

        
        if not is_valid_text(text):
            continue  
        
        post_info = {
            "í…ìŠ¤íŠ¸": text,
            "ë‚ ì§œ": entry.get("ë‚ ì§œ", ""),
            "ì¡°íšŒ": views,
            "ì¶”ì²œ": recommendations
        }
        grouped_data[company].append(post_info)
    
    return [{"ê¸°ì—…ëª…": company, "ê¸€ëª©ë¡": posts} for company, posts in grouped_data.items() if posts]

# âœ… Step 4: KOTE ê°ì„± ë¶„ì„ ì…ë ¥ ë°ì´í„° ë³€í™˜ (For_kote_input.py ê¸°ëŠ¥)
def create_kote_input(data):
    return [{"ê¸°ì—…ëª…": company_data["ê¸°ì—…ëª…"], "í…ìŠ¤íŠ¸": post["í…ìŠ¤íŠ¸"], "ë‚ ì§œ": post["ë‚ ì§œ"], "ì¡°íšŒ": post["ì¡°íšŒ"], "ì¶”ì²œ": post["ì¶”ì²œ"]} for company_data in data for post in company_data["ê¸€ëª©ë¡"]]

def run_preprocess_38(input_data, kind_data):
    
    filtered_data = filter_by_date(input_data, kind_data)
    transformed_data = transform_text(filtered_data)
    grouped_data = group_and_clean_data(transformed_data)
    kote_input_data = create_kote_input(grouped_data)

    print("âœ… ì „ì²´ ì „ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: 38_KOTE_Input.json")
    
    return kote_input_data



# âœ… ì „ì²´ ì „ì²˜ë¦¬ ì‹¤í–‰
if __name__ == "__main__":
    with open("Non_Finance_data/38/38_ver1.json", "r", encoding="utf-8") as f:
        input_data = json.load(f)
    with open("Non_Finance_data/38/KIND_38.json", "r", encoding="utf-8") as f:
        kind_data = json.load(f)
    
    filtered_data = filter_by_date(input_data, kind_data)
    transformed_data = transform_text(filtered_data)
    grouped_data = group_and_clean_data(transformed_data)
    kote_input_data = create_kote_input(grouped_data)
    
    with open("38_KOTE_Input.json", "w", encoding="utf-8") as f:
        json.dump(kote_input_data, f, ensure_ascii=False, indent=4)
    
    print("âœ… ì „ì²´ ì „ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: 38_KOTE_Input.json")

